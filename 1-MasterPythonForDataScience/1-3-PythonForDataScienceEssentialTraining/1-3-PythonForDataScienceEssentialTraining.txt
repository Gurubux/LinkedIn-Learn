https://www.linkedin.com/learning/python-for-data-science-essential-training
INTRODUCTION
WELCOME
WHAT YOU SHOULD KNOW
GETTING STARTED WITH JUPYTER

1. DATA MUNGING BASICS
FILTER AND SELECT DATA
TREAT MISSING VALUES
REMOVE DUPLICATES
CONCATENATE AND TRANSFORM DATA
GROUP AND AGGREGATE DATA

2. DATA VISUALIZATION BASICS
CREATE STANDARD LINE, BAR, AND PIE PLOTS
DEFINE PLOT ELEMENTS
FORMAT PLOTS
CREATE LABELS AND ANNOTATIONS
CREATE VISUALIZATIONS FROM TIME SERIES DATA
CONSTRUCT HISTOGRAMS, BOX PLOTS, AND SCATTER PLOTS

3. BASIC MATH AND STATISTICS
USE NUMPY ARITHMETIC
GENERATE SUMMARY STATISTICS
SUMMARIZE CATEGORICAL DATA
PARAMETRIC METHODS
NON-PARAMETRIC METHODS
TRANSFORM DATASET DISTRIBUTIONS

4. DIMENSIONALITY REDUCTION
INTRODUCTION TO MACHINE LEARNING
EXPLANATORY FACTOR ANALYSIS
PRINCIPAL COMPONENT ANALYSIS (PCA)

5. OUTLIER ANALYSIS
EXTREME VALUE ANALYSIS USING UNIVARIATE METHODS
MULTIVARIATE ANALYSIS FOR OUTLIER DETECTION
A LINEAR PROJECTION METHOD FOR MULTIVARIATE DATA

6. CLUSTER ANALYSIS
K-MEANS METHOD
HIERARCHICAL METHODS
INSTANCE-BASED LEARNING WITH K-NEAREST NEIGHBOR

7. NETWORK ANALYSIS WITH NETWORKX
INTRO TO NETWORK ANALYSIS
WORK WITH GRAPH OBJECTS
SIMULATE A SOCIAL NETWORK
GENERATE STATS ON NODES AND INSPECT GRAPHS

8. BASIC ALGORITHMIC LEARNING
LINEAR REGRESSION MODEL
LOGISTIC REGRESSION MODEL
NAÏVE BAYES CLASSIFIERS

9. WEB-BASED DATA VISUALIZATIONS WITH PLOTLY
CREATE BASIC CHARTS
CREATE STATISTICAL CHARTS
CREATE PLOTLY CHOROPLETH MAPS
CREATE PLOTLY POINT MAPS

10. WEB SCRAPING WITH BEAUTIFUL SOUP
INTRODUCTION TO BEAUTIFUL SOUP
EXPLORE NAVIGATABLESTRING OBJECTS
PARSE DATA
WEB SCRAPE IN PRACTICE

CONCLUSION

************************************************************************************************************
************************************************************************************************************
INTRODUCTION
WELCOME
WHAT YOU SHOULD KNOW
GETTING STARTED WITH JUPYTER

************************************************************************************************************
1. DATA MUNGING BASICS
************************************************************************************************************
FILTER AND SELECT DATA
TREAT MISSING VALUES
REMOVE DUPLICATES
CONCATENATE AND TRANSFORM DATA
GROUP AND AGGREGATE DATA

************************************************************************************************************
QUIZ
1. The series object series_obj has 5 items with the index labels `row 1` through `row 5`, and the integer indexes 0 through 4 respectively. The command series_obj[`row 3`] is equivalent to series_obj[2] .

2. In the dropna() DataFrame method, what will the argument axis=1 cause?
	drop columns that contain missing values, instead of rows

3. You call the method duplicated() on an object and get a True value for row 8. This means that this row is unique and has no duplicates.
	FALSE

4. What should you consider when appending a DataFrame to itself?
	Indexes will be duplicated and therefore inconsistent.

************************************************************************************************************	
************************************************************************************************************
2. DATA VISUALIZATION BASICS
************************************************************************************************************
CREATE STANDARD LINE, BAR, AND PIE PLOTS
DEFINE PLOT ELEMENTS
FORMAT PLOTS
CREATE LABELS AND ANNOTATIONS
CREATE VISUALIZATIONS FROM TIME SERIES DATA
CONSTRUCT HISTOGRAMS, BOX PLOTS, AND SCATTER PLOTS


QUIZ
1. To plot a line chart, use the plot function.
2. In your plot, you use add_axes([.5, .5, 1, 1]). Where will the axes be placed inside the figure area?
	top-right corner
3. Which argument controls the marker edge width?
	mew
4. The xy parameter is part of the annotate() function.
	TRUE
5. The Seaborn equivalent of plot(kind=`scatter`) is regplot() .


************************************************************************************************************
3. BASIC MATH AND STATISTICS
************************************************************************************************************
----------------------------------------------------------------------------------------------------------------------
USE NUMPY ARITHMETIC
Creating arrays
np.random.seed(25)
np.random.randn(6) #array([ 0.23,  1.03, -0.84, -0.59, -0.96, -0.22])
c = 36*np.random.randn(6) #array([ 38.04, -15.11,  82.61, -93.4 , 101.62,  24.51])
d = np.arange(1,35) #array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34])
arthimetic on arrays
a * 10
c + a
c - a
c * a
c / a
Multiplying matrices
aa*bb
np.dot(aa,bb)

----------------------------------------------------------------------------------------------------------------------
GENERATE SUMMARY STATISTICS-(DESCRIPTIVE)
decribe a variable`s numeric values
cars.sum()
cars.sum(axis=1)
cars.median()
cars.mean()
cars.max()
mpg.idxmax()

describe variable distribution
cars.std()
cars.var()
cars.gear.value_counts()
cars.describe()


----------------------------------------------------------------------------------------------------------------------
SUMMARIZE CATEGORICAL DATA
cars.carb.value_counts()
cars.describe()

# describe with group by column,the  mean mediean mode, 5,50,75% count min max for each other column.
gears_group = cars_cat.groupby(`gear`)
gears_group.describe()

Convert int into category
pd.Series(cars.gear, dtype="category")

categorical data with crosstabs
pd.crosstab(cars[`am`], cars[`gear`])


----------------------------------------------------------------------------------------------------------------------
PARAMETRIC METHODS
-Exploring correlation between variables
	- parametric methods in pandas and scipy
		-The Pearson Correlation
			sb.pairplot(cars)
			pd.plotting.scatter_matrix(mouse); 

			import scipy
			from scipy.stats.stats import pearsonr
			pearsonr_coefficient, p_value = pearsonr(mpg, hp)
			print(`mpg, hp PearsonR Correlation Coefficient %0.3f` % (pearsonr_coefficient))
			print(f`mpg, hp p_value {p_value:.7f}`)
			#calculate the Pearson correlation coefficient
			corr = X.corr()
			#visualize the Pearson correlation coefficient
			sb.heatmap(corr,xticklabels=corr.columns.values, yticklabels=corr.columns.values)

		Pearson Correlation Assumptions
		- Data is normally distributed
		- You have continous, numeric values
		- Your variables are linearly related
----------------------------------------------------------------------------------------------------------------------
NON-PARAMETRIC METHODS
	- Used for Categorical, Non Linearly related, Non Normally distributed variables
		1. Spearman`s rank  correlation - Ordinale Data types- Numerical variables that can be categorized -->  1 0 -1
			Spearman`s rank Correlation Assumptions
				a. Ordinal Variables ( Numeric but may be ranked like a categorical variable)
				b. Related Non Linearly
				c. non-normally distributed
			
			from scipy.stats import spearmanr
			spearmanr_coefficient, p_value = spearmanr(cyl, vs)
			print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
			>>> Spearman Rank Correlation Coefficient -0.814  					STRONG Negative Correlation

			spearmanr_coefficient, p_value = spearmanr(cyl, am)
			print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
			Spearman Rank Correlation Coefficient -0.522    							Negative Correlation

			spearmanr_coefficient, p_value = spearmanr(cyl, gear)
			print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
			Spearman Rank Correlation Coefficient -0.564   								Negative Correlation



		2. Chi-Square tables - Test for independence between variables -  Null hypothesis= Variables are independent.
			p < 0.5 - Reject Null hypothesis and conclude that the variables are correlated
			p > 0.5 - Accept Null hypothesis and conclude that the variables are "INDEPENDENT"
			Chi-Square Correlation Assumptions
				a. Make sure if variables are categorical or Numeric
				b. If numerical, make sure you have binned them( variable has numeric values 0 - 100, bin them in bins of 10 like 0-10, 11-20, ...91-100 )

			table = pd.crosstab(cyl, am)
			from scipy.stats import chi2_contingency

			chi2, p, dof, expected = chi2_contingency(table.values)
			print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
			>>>Chi-square Statistic 8.741 p_value 0.013

			table = pd.crosstab(cars[`cyl`], cars[`vs`])
			chi2, p, dof, expected = chi2_contingency(table.values)
			print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
			>>>Chi-square Statistic 21.340 p_value 0.000


			table = pd.crosstab(cars[`cyl`], cars[`gear`])
			chi2, p, dof, expected = chi2_contingency(table.values)
			print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
			>>>Chi-square Statistic 18.036 p_value 0.001

			"p < 0.05 so Reject the Null Hypothesis and conclude that variables are correlated and not independent"

----------------------------------------------------------------------------------------------------------------------
TRANSFORM DATASET DISTRIBUTIONS
1. Normalization - 			Value of Observation
				  -------------------------------------
					Sum of All Observation in variable

2. Standardization - Rescaling Data so it has a zero mean and unit Variance

- Normalizing and transforming features with MinMaxScalar() and fit_transform()
import sklearn
from sklearn import preprocessing
mpg_matrix = mpg.values.reshape(-1,1)
		>>>
		array([21. , 21. , 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,
		       16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5,
		       15.2, 13.3, 19.2, 27.3, 26. , 30.4, 15.8, 19.7, 15. , 21.4])
		to 
		array([[21. ],[21. ],[22.8],[21.4],[18.7],[18.1],[14.3],[24.4],[22.8],[19.2],[17.8],[16.4],[17.3],[15.2],[10.4],[10.4],[14.7],[32.4],[30.4],[33.9],[21.5],[15.5],[15.2],[13.3],[19.2],[27.3],[26. ],[30.4],[15.8],[19.7],[15. ],[21.4]])

scaled = preprocessing.MinMaxScaler()
scaled_mpg = scaled.fit_transform(mpg_matrix)
		>>>
		array([[0.45106383],[0.45106383],[0.52765957],[0.46808511],[0.35319149],[0.32765957],[0.16595745],[0.59574468],[0.52765957],[0.37446809],[0.31489362],[0.25531915],[0.29361702],[0.20425532],[0.        ],[0.        ],[0.18297872],[0.93617021],[0.85106383],[1.        ],[0.47234043],[0.21702128],[0.20425532],[0.12340426],[0.37446809],[0.71914894],[0.66382979],[0.85106383],[0.22978723],[0.39574468],[0.19574468],[0.46808511]])
plt.plot(scaled_mpg)

# Set Y values Range instead of default 0 to 1
mpg_matrix = mpg.values.reshape(-1,1)
scaled = preprocessing.MinMaxScaler(feature_range=(0,10))
scaled_mpg = scaled.fit_transform(mpg_matrix)
plt.plot(scaled_mpg)


-  Using scale() to scale your features
from sklearn.preprocessing import scale
standardized_mpg = scale(mpg, axis=0, with_mean=False, with_std=False) # Y axis is same 
plt.plot(standardized_mpg) 

standardized_mpg = scale(mpg) # Y axis is changed into - and + values
plt.plot(standardized_mpg)




----------------------------------------------------------------------------------------------------------------------
QUIZ
1. You have two 2x2 matrices; one has 1`s for all values, and the other has 2`s for all values. Their NumPy dot multiplication will produce a 2x2 matrix with 4`s for all values .
3. Which Pearson correlation requirement will not be satisfied when analyzing the correlation between people`s height and age?
	The relationship is linear.
	"Non-linear relationships have an apparent pattern, just not linear. For example, as age increases height increases up to a point then levels off after reaching a maximum height."
4. In which way is the Spearman`s Rank analysis similar to Pearson`s?
	It produces R-values between -1 and 1

************************************************************************************************************
4. DIMENSIONALITY REDUCTION
************************************************************************************************************
INTRODUCTION TO MACHINE LEARNING
"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch04/ML_InThisCourse.PNG"

----------------------------------------------------------------------------------------------------------------------
EXPLANATORY FACTOR ANALYSIS
Factor Analysis Assumptions 
1. you`re features are metric,
2. they are either continuous or ordinal,
3. you have a correlation coefficient R greater than 0.3,
4. you have more than 100 observations, and more than five observations per feature. 
5. It also assumes that you`re sample is homogenous. 

--------------------------------CODE- START-----------------------------
iris = datasets.load_iris()
X = iris.data
variable_names = iris.feature_names

factor = FactorAnalysis().fit(X)

pd.DataFrame(factor.components_, columns=variable_names)
	sepal length	sepal width 	petal length	petal width
0	0.707227		-0.153147		1.653151		0.701569
1	0.114676		0.159763		-0.045604		-0.014052
2	-0.000000		0.000000		0.000000		0.000000
3	-0.000000		0.000000		0.000000		-0.000000
--------------------------------CODE - END------------------------------

----------------------------------------------------------------------------------------------------------------------	
PRINCIPAL COMPONENT ANALYSIS (PCA)

Singular Value Decomposition-(SVD) is a linear algebra method that you use to decompose a matrix into three resultant matrices. You do this in order to reduce information redundancy and noise. SVD is most commonly used for principle component analysis.

This is our original data set, it`s called A, and we decompose it into three resultant matrices, U, S and V. 
	U is the "left  orthogonal matrix", and it holds all of the important non-information-redundant information about the observations in the original data set. 
	V is the "right orthogonal matrix", and it holds all of the important non-redundant information on features in the original data set. 
	S, this is the "diagonal matrix", and it contains all of the information about the decomposition processes that were performed during the compression.

PCA USE-CASES :
	You can use PCA for 
		fraud detection, 
		spam detection, 
		image recognition, 
		speech recognition, or 
		outlier removal, if you are using PCA for data pre-processing. 

----------------------------CODE- START------------------------------------------
iris_pca = PCA().fit_transform(X)

pca.explained_variance_ratio_ #EXPLAINED VARIANCE RATION
array([0.92461621, 0.05301557, 0.01718514, 0.00518309])
"Look at the explained variance ratio. We see that the first component explains 92.4% of the data set`s variation. That means it holds 92.4% of the data`s information in one principal component. Pretty cool, right? And by taking the first two components, we only elude 2.3% of the data set`s information. That`s the junk we want to get rid of anyway, so let`s do that. We will take only the first two components and feel satisfied knowing that they contain 97.7% of the iris data set`s original information. "

pca.explained_variance_ratio_.sum() #CUMMULATIVE VARIANCE RATION
1.0

comps = pd.DataFrame(pca.components_, columns=variable_names)

	sepal length (cm)	sepal width (cm)	petal length (cm)	petal width (cm)
0	0.361590				-0.082269			0.856572			0.358844
1	0.656540				0.729712			-0.175767			-0.074706
2	-0.580997				0.596418			0.072524			0.549061
3	0.317255				-0.324094			-0.479719			0.751121
--------------------------CODE- END--------------------------------------------------


PCA and FACTOR Analysis Usage in MACHINE LEARNING
"YOU MAY BE WONDERING HOW YOU CAN USE FACTORS AND COMPONENTS. WELL, BOTH FACTORS AND COMPONENTS REPRESENT WHAT IS LEFT OF THE DATA SET AFTER INFORMATION REDUNDANCY AND NOISE HAVE BEEN STRIPPED OUT. YOU USE THEM AS INPUT VARIABLES FOR MACHINE LEARNING ALGORITHMS TO GENERATE PREDICTIONS FROM THESE COMPRESSED REPRESENTATIONS OF YOUR DATA."

 The results from this correlation heat map show that, 
 1. principal component one is strongly positively correlated with petal length and moderately positively correlated with sepal length and petal width. Component one is slightly negatively correlated with sepal width,  
 2. principal component number two is strongly negatively correlated with sepal length and sepal width, and slightly negatively correlated with petal length and petal width. You may be wondering how you can use these components once you`ve isolated. Well, you can use them as input variables for machine learning algorithms. So in the case of the iris data set, you could use the two components we`ve generated as input for a logistic regression model to predict species labels for new incoming data points. 

----------------------------------------------------------------------------------------------------------------------
QUIZ
1. How much of your data should go to the test set?
	one-third
2. The goal of PCA is to keep only the principal components that matter. 70%
************************************************************************************************************
************************************************************************************************************
5. OUTLIER ANALYSIS
************************************************************************************************************
EXTREME VALUE ANALYSIS USING UNIVARIATE METHODS
Types of Outliers
1. Point Outliers - Univariate Outliers
2. Contextual Outliers - Location,Temperature
3. Collective Outliers - These are a collection of observations that are anomalous but appear close to one another because they all have similar anomalous values. - DBSCAN
			(anomalous - deviating from what is standard, normal, or expected.)

Use of Outlier detection( Basically used to uncover anomalies in data)
	1. equipment failure notification, 
	2. fraud detection, and 
	3. cybersecurity event notification
----------------------------------------------------------------------------------------------------------------------
UNIVARIATE METHODS: 
	Tukey Boxlplots - You can detect unusually high or low data points in a variable by applying the Tukey methods for outlier detection
		Boxplot whiskers - IQR Inter Quantile Range
		"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch05/05_01/Univariate_Boxplot_Tukey.PNG"
df.boxplot(return_type='dict')
plt.plot()

def tukey_label(data):
    IQR = np.quantile(data, 0.75) - np.quantile(data, 0.25)
    Q1 = np.quantile(data, 0.25)
    Q3 = np.quantile(data, 0.75)
    #print(f"{IQR:.2f},{Q1:.2f},{Q3:.2f}")
    outlier_1 = Q1 - (IQR * 1.5)
    outlier_2 = Q3 + (IQR * 1.5)
    return (outlier_1,outlier_2)

Sepal_Width = X[:,1]
iris_outliers = (Sepal_Width > 4)
df[iris_outliers]

Sepal_Width = X[:,1]
iris_outliers = (Sepal_Width < 2.05)
df[iris_outliers]
"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch05/05_01/05_01.ipynb"


----------------------------------------------------------------------------------------------------------------------
MULTIVARIATE ANALYSIS FOR OUTLIER DETECTION
Box-plot
df.columns=['Sepal Length','Sepal Width','Petal Length','Petal Width', 'Species']
data = df.iloc[:,0:4].values
target = df.iloc[:,4].values

sb.boxplot(x='Species', y='Sepal Length', data=df, palette='hls')

pair-plot
sb.pairplot(df, hue='Species', palette='hls')


----------------------------------------------------------------------------------------------------------------------
A LINEAR PROJECTION METHOD FOR MULTIVARIATE DATA
from sklearn.cluster import DBSCAN
model = DBSCAN(eps=0.8, min_samples=19).fit(data)
print(model)
>>> DBSCAN(algorithm='auto', eps=0.8, leaf_size=30, metric='euclidean',
    metric_params=None, min_samples=19, n_jobs=1, p=None)

outliers_df = pd.DataFrame(data)
print(model.labels_)
>>>
[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1 -1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1
  1  1 -1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1]


from collections import Counter
print(Counter(model.labels_))
>>>Counter({1: 94, 0: 50, -1: 6})

#Predicted Outliers
print(outliers_df[model.labels_ ==-1])
       0    1    2    3
98   5.1  2.5  3.0  1.1
105  7.6  3.0  6.6  2.1
117  7.7  3.8  6.7  2.2
118  7.7  2.6  6.9  2.3
122  7.7  2.8  6.7  2.0
131  7.9  3.8  6.4  2.0

#Plotting DBSCAN Predictions using Scatter plot

fig = plt.figure()
ax = fig.add_axes([.1, .1, 1, 1]) 

colors = model.labels_

ax.scatter(data[:,2], data[:,1], c=colors, s=120)#Petal Length , Sepal width
ax.set_xlabel('Petal Length')
ax.set_ylabel('Sepal Width')
plt.title('DBScan for Outlier Detection')

DBSCAN Assumptions:
Just make sure that the number of outliers you choose is less than 5% of the total number of observations in your dataset.You do that by adjusting your model parameters accordingly. 

DBSCAN Parameters:
eps and min_samples. 
	"Eps" sets the maximum distance between two samples for them to be clustered in the same neighborhood. You want to start with an eps value of zero point one. 
	As far as "min_samples", this is the minimum number of samples in a neighborhood for a data point to qualify as a core point. Again, here, you want to start with a very low sample size. You adjust your parameters until you've got just less than 5% of your total dataset size, labeled as outliers.

For Example: 
model = DBSCAN(eps=0.8, min_samples=19).fit(data)
and
model = DBSCAN(eps=1, min_samples=30).fit(data)
Produces 6 Outliers which is less that 5%(i.e 7.5) of total sample size i.e 150


----------------------------------------------------------------------------------------------------------------------
QUIZ
1. Using Tukey outlier labeling, the lower quartile ends at 3.6 and the upper quartile starts at 4.8. Which of the following is not an outlier?
6.5
	IQR = 4.8 - 3.6 = 1.2
    Q1 = 3.6
    Q3 = 4.8
    #print(f"{IQR:.2f},{Q1:.2f},{Q3:.2f}")
    outlier_1 = Q1 - (IQR * 1.5) = 3.6 - (1.2*1.5) = 1.8  # lesser that 1.8 is a outlier
    outlier_2 = Q3 + (IQR * 1.5)  = 4.8 + (1.2*1.5) = 6.6‬ # Greater that 6.6‬ is a outlier
************************************************************************************************************
6. CLUSTER ANALYSIS
************************************************************************************************************
K-MEANS METHOD - \UNSUPERVISED
Used to predict Subgroups from withiin a dataset.
Predictions are based on : 
	1. Number of cluster centers that are present,T
	2. The nearest mean values between observations.(Measured using euclidean distance between observations)
Use-Cases : 
	1. Market price and cost modeling, 
	2. Customer segmentation, 
	3. Hedge fund classification, 
	4. Insurance claim fraud detection.
Assumptions : 
	1. the first thing is you always need to "scale your variables" before clustering your data,  - scale(iris.data) 
	2. you need to look at a scatter plot or a data table to estimate the "number of cluster centers" to set for the k parameter in the model. n_clusters=3
Evaluation : 
	classification_report(y, relabel)

------------------------CODE START------------------------
from sklearn.cluster import KMeans
clustering = KMeans(n_clusters=3, random_state=5)
clustering.fit(X)
y_pred = clustering.labels_

Plotting K-MEANS
color_theme = np.array(['darkgray', 'lightsalmon', 'powderblue'])

plt.subplot(1,2,1)
plt.scatter(x=iris_df.Petal_Length,y=iris_df.Petal_Width, c=color_theme[iris.target], s=50)
plt.title('Ground Truth Classification')

plt.subplot(1,2,2)
plt.scatter(x=iris_df.Petal_Length,y=iris_df.Petal_Width, c=color_theme[clustering.labels_], s=50)
plt.title('K-Means Classification')

Evaluating K-MEANS
print(classification_report(y, relabel))
------------------------CODE END------------------------

----------------------------------------------------------------------------------------------------------------------
HIERARCHICAL METHODS - \UNSUPERVISED
Hierarchical clustering is an unsupervised machine learning method that you can use to predict subgroups based on 
	1. the "Distance" between data points and their nearest neighbors.
	2. Each data point is "linked" to its neighbor that is most nearby according to the distance metric that you choose.
(Hierarchical clustering predicts subgroups within data by finding the distance between each data point and its nearest neighbor and also linking up the most nearby neighbors.)
DENDROGRAM
	You can find the number of subgroups(like number or cluster centers in K-Means) that are appropriate for a hierarchical clustering model by looking at a dendrogram. 
	A dendrogram is a tree graph that`s useful for visually displaying taxonomies, lineages, and relatedness. 
	"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch06/06_02/Hierarchical_Clustering_Dendogram.PNG"
Use-Cases : 
	1. hospital resource management, 
	2. business process management, 
	3. customer segmentation analysis, 
	4. social network analysis. 
Assumptions : 
	1. Number of Centroids - ( Using dendrogram)
	2. Maximum distance between points = 150 for example.
Dendrogram Use example: 
	Imagine that you only want a maximum distance between a point and its nearest neighbor. You want that maximum distance to be 150. You would plot out your dendrogram, and then you would set a line where y=150. Then along that line, you look at each point where your dendrogram intersects the y=150 line, so you have one, two, three, four, five. If you wanted to have a maximum distance of 150 between each point and its nearest neighbor, you would have five cluster centers. You could also set the maximum distance between points to be 500, an in that case you would have two cluster centers.
	"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch06/06_02/Hierarchical_Clustering_Dendogram_Example.PNG"
Parameter : 
	1. Distance Metrics  : Distance metrics can be set as either Euclidean, Manhattan, or Cosine
	2. Linkage 			 : The linkage parameters are Ward, Complete, and Average. 
	(Use trial and error )

------------------------CODE START------------------------
from scipy.cluster.hierarchy import dendrogram, linkage
Z = linkage(X, 'ward')
# Z is the clustering results that have been generated from the Scipy hierarchical clustering algorithm. 
dendrogram(Z, truncate_mode='lastp', p=12, leaf_rotation=45., leaf_font_size=15., show_contracted=True)

plt.title('Truncated Hierarchical Clustering Dendrogram')
plt.xlabel('Cluster Size')
plt.ylabel('Distance')


plt.axhline(y=500) # if max distance = 500 then Clusters = 2
plt.axhline(y=150) # if max distance = 150 then Clusters = 5
plt.show()


#  With 500 set as our maximum distance between nearest neighbors, based on the dendrogram, we have two clusters. 
# Also, the Target variable AM feature assums two possible outcomes, 0 or 1 so k =2 
k=2

Hclustering = AgglomerativeClustering(n_clusters=k, affinity='euclidean', linkage='ward')
Hclustering.fit(X)

sm.accuracy_score(y, Hclustering.labels_)
>>>0.78125

					@NOTE - "Ward" can only work with "euclidean" distances. 
					Results of 
						ecludiean and Ward, average,Complete
						manhattan and 		average,Complete
						cosine    and 		average,Complete

------------------------CODE END------------------------

----------------------------------------------------------------------------------------------------------------------
INSTANCE-BASED LEARNING WITH K-NEAREST NEIGHBOR \SUPERVISED
K-nearest neighbor works by memorizing observations within a labeled Training set to predict classification labels for new, incoming, unlabeled observations. The algorithm makes predictions based on how similar training observations are to the new, incoming observations. The more similar the observation's value, the more likely they will be classified with the same label.

Use-Cases : 
	1. stock price prediction, 
	2. recommendation systems, 
	3. predictive trip planning, 
	4. credit risk analysis. 

Assumptions : 
	1. the data set has little noise, 
	2. it`s labeled, 
	3. it contains only relevant features, 
	4. the data set has distinguishable sub groups.
	5. Small or medium sized Data sets

**NOTE**
	Takes too Long to classify for large datasets so avoid for that case.



------------------------CODE START------------------------
#Modelling of KNN
clf = neighbors.KNeighborsClassifier()
clf.fit(X_train, y_train)
print(clf)
	>>> KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights='uniform')

#Prediction of KNN
y_pred = clf.predict(X_test)

#Evaluation of KNN
metrics.classification_report(y_test, y_pred)
>>>
	             precision    recall  f1-score   support

          0       0.71      1.00      0.83         5
          1       1.00      0.67      0.80         6

avg / total       0.87      0.82      0.82        11


sm.accuracy_score(y_expect, y_pred)
>>>	0.84375

#Plotting KNN Classification
color_theme = np.array(['red', 'blue'])

plt.subplot(1,2,1)
plt.scatter(x=X[:,0],y=X[:,1], c=color_theme[y], s=50)
plt.title('Ground Truth Classification')

plt.subplot(1,2,2)
plt.scatter(x=X[:,0],y=X[:,1], c=color_theme[y_], s=50)
plt.title('K-Nearest Neighbor')

------------------------CODE END------------------------


QUIZ
1. The classification_report() function can be used to evaluate the accuracy of the K-means clustering analysis.
2. FALSE  - The k-NN model is especially useful for very large datasets. (Takes too long for Large datasets, so avoid.)



************************************************************************************************************
7. NETWORK ANALYSIS WITH NETWORKX
************************************************************************************************************
INTRO TO NETWORK ANALYSIS
Uses of Network Analysis
	1. Social Media Marketing Strategy
	2. Infrastructure system design, 
	3. Financial risk management, 
	4. Public health management.

Difference Network and Graph
	Network 
		A body of connected data that is evaluated during graph analysis 
	Graph 
		The actually data "visualization" schematic that depicts the network data. 
"Nodes" are vertices around which the graph is formed and 
"Edges" are the lines that connect them.
Directed "graph", also known as a digraph, is a graph where there is a direction assigned to each edge that connects the nodes. 
Directed "edge" is an edge feature that has been a assigned direction between those nodes.
----------------------------------------------------------------------------------------------------------------------
WORK WITH GRAPH OBJECTS
----------------------------------------------------------------------------------------------------------------------
SIMULATE A SOCIAL NETWORK
----------------------------------------------------------------------------------------------------------------------
GENERATE STATS ON NODES AND INSPECT GRAPHS
----------------------------------------------------------------------------------------------------------------------

************************************************************************************************************
8. BASIC ALGORITHMIC LEARNING
************************************************************************************************************
LINEAR REGRESSION MODEL
----------------------------------------------------------------------------------------------------------------------
LOGISTIC REGRESSION MODEL
----------------------------------------------------------------------------------------------------------------------
NAÏVE BAYES CLASSIFIERS
----------------------------------------------------------------------------------------------------------------------

************************************************************************************************************
9. WEB-BASED DATA VISUALIZATIONS WITH PLOTLY
************************************************************************************************************
CREATE BASIC CHARTS
----------------------------------------------------------------------------------------------------------------------
CREATE STATISTICAL CHARTS
----------------------------------------------------------------------------------------------------------------------
CREATE PLOTLY CHOROPLETH MAPS
----------------------------------------------------------------------------------------------------------------------
CREATE PLOTLY POINT MAPS
----------------------------------------------------------------------------------------------------------------------

************************************************************************************************************
10. WEB SCRAPING WITH BEAUTIFUL SOUP
************************************************************************************************************
INTRODUCTION TO BEAUTIFUL SOUP
----------------------------------------------------------------------------------------------------------------------
EXPLORE NAVIGATABLESTRING OBJECTS
----------------------------------------------------------------------------------------------------------------------
PARSE DATA
----------------------------------------------------------------------------------------------------------------------
WEB SCRAPE IN PRACTICE
----------------------------------------------------------------------------------------------------------------------

************************************************************************************************************
CONCLUSION
************************************************************************************************************