https://www.linkedin.com/learning/python-for-data-science-essential-training
INTRODUCTION
WELCOME
WHAT YOU SHOULD KNOW
GETTING STARTED WITH JUPYTER

1. DATA MUNGING BASICS
FILTER AND SELECT DATA
TREAT MISSING VALUES
REMOVE DUPLICATES
CONCATENATE AND TRANSFORM DATA
GROUP AND AGGREGATE DATA

2. DATA VISUALIZATION BASICS
CREATE STANDARD LINE, BAR, AND PIE PLOTS
DEFINE PLOT ELEMENTS
FORMAT PLOTS
CREATE LABELS AND ANNOTATIONS
CREATE VISUALIZATIONS FROM TIME SERIES DATA
CONSTRUCT HISTOGRAMS, BOX PLOTS, AND SCATTER PLOTS

3. BASIC MATH AND STATISTICS
USE NUMPY ARITHMETIC
GENERATE SUMMARY STATISTICS
SUMMARIZE CATEGORICAL DATA
PARAMETRIC METHODS
NON-PARAMETRIC METHODS
TRANSFORM DATASET DISTRIBUTIONS

4. DIMENSIONALITY REDUCTION
INTRODUCTION TO MACHINE LEARNING
EXPLANATORY FACTOR ANALYSIS
PRINCIPAL COMPONENT ANALYSIS (PCA)

5. OUTLIER ANALYSIS
EXTREME VALUE ANALYSIS USING UNIVARIATE METHODS
MULTIVARIATE ANALYSIS FOR OUTLIER DETECTION
A LINEAR PROJECTION METHOD FOR MULTIVARIATE DATA

6. CLUSTER ANALYSIS
K-MEANS METHOD
HIERARCHICAL METHODS
INSTANCE-BASED LEARNING WITH K-NEAREST NEIGHBOR

7. NETWORK ANALYSIS WITH NETWORKX
INTRO TO NETWORK ANALYSIS
WORK WITH GRAPH OBJECTS
SIMULATE A SOCIAL NETWORK
GENERATE STATS ON NODES AND INSPECT GRAPHS

8. BASIC ALGORITHMIC LEARNING
LINEAR REGRESSION MODEL
LOGISTIC REGRESSION MODEL
NAÏVE BAYES CLASSIFIERS

9. WEB-BASED DATA VISUALIZATIONS WITH PLOTLY
CREATE BASIC CHARTS
CREATE STATISTICAL CHARTS
CREATE PLOTLY CHOROPLETH MAPS
CREATE PLOTLY POINT MAPS

10. WEB SCRAPING WITH BEAUTIFUL SOUP
INTRODUCTION TO BEAUTIFUL SOUP
EXPLORE NAVIGATABLESTRING OBJECTS
PARSE DATA
WEB SCRAPE IN PRACTICE

CONCLUSION

************************************************************************************************************
************************************************************************************************************
INTRODUCTION
WELCOME
WHAT YOU SHOULD KNOW
GETTING STARTED WITH JUPYTER

************************************************************************************************************
1. DATA MUNGING BASICS
************************************************************************************************************
FILTER AND SELECT DATA
TREAT MISSING VALUES
REMOVE DUPLICATES
CONCATENATE AND TRANSFORM DATA
GROUP AND AGGREGATE DATA

************************************************************************************************************
QUIZ
1. The series object series_obj has 5 items with the index labels `row 1` through `row 5`, and the integer indexes 0 through 4 respectively. The command series_obj[`row 3`] is equivalent to series_obj[2] .

2. In the dropna() DataFrame method, what will the argument axis=1 cause?
	drop columns that contain missing values, instead of rows

3. You call the method duplicated() on an object and get a True value for row 8. This means that this row is unique and has no duplicates.
	FALSE

4. What should you consider when appending a DataFrame to itself?
	Indexes will be duplicated and therefore inconsistent.

************************************************************************************************************	
************************************************************************************************************
2. DATA VISUALIZATION BASICS
************************************************************************************************************
CREATE STANDARD LINE, BAR, AND PIE PLOTS
DEFINE PLOT ELEMENTS
FORMAT PLOTS
CREATE LABELS AND ANNOTATIONS
CREATE VISUALIZATIONS FROM TIME SERIES DATA
CONSTRUCT HISTOGRAMS, BOX PLOTS, AND SCATTER PLOTS


QUIZ
1. To plot a line chart, use the plot function.
2. In your plot, you use add_axes([.5, .5, 1, 1]). Where will the axes be placed inside the figure area?
	top-right corner
3. Which argument controls the marker edge width?
	mew
4. The xy parameter is part of the annotate() function.
	TRUE
5. The Seaborn equivalent of plot(kind=`scatter`) is regplot() .


************************************************************************************************************
3. BASIC MATH AND STATISTICS
************************************************************************************************************
----------------------------------------------------------------------------------------------------------------------
USE NUMPY ARITHMETIC
Creating arrays
np.random.seed(25)
np.random.randn(6) #array([ 0.23,  1.03, -0.84, -0.59, -0.96, -0.22])
c = 36*np.random.randn(6) #array([ 38.04, -15.11,  82.61, -93.4 , 101.62,  24.51])
d = np.arange(1,35) #array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34])
arthimetic on arrays
a * 10
c + a
c - a
c * a
c / a
Multiplying matrices
aa*bb
np.dot(aa,bb)

----------------------------------------------------------------------------------------------------------------------
GENERATE SUMMARY STATISTICS-(DESCRIPTIVE)
decribe a variable`s numeric values
cars.sum()
cars.sum(axis=1)
cars.median()
cars.mean()
cars.max()
mpg.idxmax()

describe variable distribution
cars.std()
cars.var()
cars.gear.value_counts()
cars.describe()


----------------------------------------------------------------------------------------------------------------------
SUMMARIZE CATEGORICAL DATA
cars.carb.value_counts()
cars.describe()

# describe with group by column,the  mean mediean mode, 5,50,75% count min max for each other column.
gears_group = cars_cat.groupby(`gear`)
gears_group.describe()

Convert int into category
pd.Series(cars.gear, dtype="category")

categorical data with crosstabs
pd.crosstab(cars[`am`], cars[`gear`])


----------------------------------------------------------------------------------------------------------------------
PARAMETRIC METHODS
-Exploring correlation between variables
	- parametric methods in pandas and scipy
		-The Pearson Correlation
			sb.pairplot(cars)
			pd.plotting.scatter_matrix(mouse); 

			import scipy
			from scipy.stats.stats import pearsonr
			pearsonr_coefficient, p_value = pearsonr(mpg, hp)
			print(`mpg, hp PearsonR Correlation Coefficient %0.3f` % (pearsonr_coefficient))
			print(f`mpg, hp p_value {p_value:.7f}`)
			#calculate the Pearson correlation coefficient
			corr = X.corr()
			#visualize the Pearson correlation coefficient
			sb.heatmap(corr,xticklabels=corr.columns.values, yticklabels=corr.columns.values)

		Pearson Correlation Assumptions
		- Data is normally distributed
		- You have continous, numeric values
		- Your variables are linearly related
----------------------------------------------------------------------------------------------------------------------
NON-PARAMETRIC METHODS
	- Used for Categorical, Non Linearly related, Non Normally distributed variables
		1. Spearman`s rank  correlation - Ordinale Data types- Numerical variables that can be categorized -->  1 0 -1
			Spearman`s rank Correlation Assumptions
				a. Ordinal Variables ( Numeric but may be ranked like a categorical variable)
				b. Related Non Linearly
				c. non-normally distributed
			
			from scipy.stats import spearmanr
			spearmanr_coefficient, p_value = spearmanr(cyl, vs)
			print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
			>>> Spearman Rank Correlation Coefficient -0.814  					STRONG Negative Correlation

			spearmanr_coefficient, p_value = spearmanr(cyl, am)
			print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
			Spearman Rank Correlation Coefficient -0.522    							Negative Correlation

			spearmanr_coefficient, p_value = spearmanr(cyl, gear)
			print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
			Spearman Rank Correlation Coefficient -0.564   								Negative Correlation



		2. Chi-Square tables - Test for independence between variables -  Null hypothesis= Variables are independent.
			p < 0.5 - Reject Null hypothesis and conclude that the variables are correlated
			p > 0.5 - Accept Null hypothesis and conclude that the variables are "INDEPENDENT"
			Chi-Square Correlation Assumptions
				a. Make sure if variables are categorical or Numeric
				b. If numerical, make sure you have binned them( variable has numeric values 0 - 100, bin them in bins of 10 like 0-10, 11-20, ...91-100 )

			table = pd.crosstab(cyl, am)
			from scipy.stats import chi2_contingency

			chi2, p, dof, expected = chi2_contingency(table.values)
			print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
			>>>Chi-square Statistic 8.741 p_value 0.013

			table = pd.crosstab(cars[`cyl`], cars[`vs`])
			chi2, p, dof, expected = chi2_contingency(table.values)
			print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
			>>>Chi-square Statistic 21.340 p_value 0.000


			table = pd.crosstab(cars[`cyl`], cars[`gear`])
			chi2, p, dof, expected = chi2_contingency(table.values)
			print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
			>>>Chi-square Statistic 18.036 p_value 0.001

			"p < 0.05 so Reject the Null Hypothesis and conclude that variables are correlated and not independent"

----------------------------------------------------------------------------------------------------------------------
TRANSFORM DATASET DISTRIBUTIONS
1. Normalization - 			Value of Observation
				  -------------------------------------
					Sum of All Observation in variable

2. Standardization - Rescaling Data so it has a zero mean and unit Variance

- Normalizing and transforming features with MinMaxScalar() and fit_transform()
import sklearn
from sklearn import preprocessing
mpg_matrix = mpg.values.reshape(-1,1)
		>>>
		array([21. , 21. , 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,
		       16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5,
		       15.2, 13.3, 19.2, 27.3, 26. , 30.4, 15.8, 19.7, 15. , 21.4])
		to 
		array([[21. ],[21. ],[22.8],[21.4],[18.7],[18.1],[14.3],[24.4],[22.8],[19.2],[17.8],[16.4],[17.3],[15.2],[10.4],[10.4],[14.7],[32.4],[30.4],[33.9],[21.5],[15.5],[15.2],[13.3],[19.2],[27.3],[26. ],[30.4],[15.8],[19.7],[15. ],[21.4]])

scaled = preprocessing.MinMaxScaler()
scaled_mpg = scaled.fit_transform(mpg_matrix)
		>>>
		array([[0.45106383],[0.45106383],[0.52765957],[0.46808511],[0.35319149],[0.32765957],[0.16595745],[0.59574468],[0.52765957],[0.37446809],[0.31489362],[0.25531915],[0.29361702],[0.20425532],[0.        ],[0.        ],[0.18297872],[0.93617021],[0.85106383],[1.        ],[0.47234043],[0.21702128],[0.20425532],[0.12340426],[0.37446809],[0.71914894],[0.66382979],[0.85106383],[0.22978723],[0.39574468],[0.19574468],[0.46808511]])
plt.plot(scaled_mpg)

# Set Y values Range instead of default 0 to 1
mpg_matrix = mpg.values.reshape(-1,1)
scaled = preprocessing.MinMaxScaler(feature_range=(0,10))
scaled_mpg = scaled.fit_transform(mpg_matrix)
plt.plot(scaled_mpg)


-  Using scale() to scale your features
from sklearn.preprocessing import scale
standardized_mpg = scale(mpg, axis=0, with_mean=False, with_std=False) # Y axis is same 
plt.plot(standardized_mpg) 

standardized_mpg = scale(mpg) # Y axis is changed into - and + values
plt.plot(standardized_mpg)




----------------------------------------------------------------------------------------------------------------------
QUIZ
1. You have two 2x2 matrices; one has 1`s for all values, and the other has 2`s for all values. Their NumPy dot multiplication will produce a 2x2 matrix with 4`s for all values .
3. Which Pearson correlation requirement will not be satisfied when analyzing the correlation between people`s height and age?
	The relationship is linear.
	"Non-linear relationships have an apparent pattern, just not linear. For example, as age increases height increases up to a point then levels off after reaching a maximum height."
4. In which way is the Spearman`s Rank analysis similar to Pearson`s?
	It produces R-values between -1 and 1

************************************************************************************************************
4. DIMENSIONALITY REDUCTION
************************************************************************************************************
INTRODUCTION TO MACHINE LEARNING
"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch04/ML_InThisCourse.PNG"

----------------------------------------------------------------------------------------------------------------------
EXPLANATORY FACTOR ANALYSIS
Factor Analysis Assumptions 
1. you`re features are metric,
2. they are either continuous or ordinal,
3. you have a correlation coefficient R greater than 0.3,
4. you have more than 100 observations, and more than five observations per feature. 
5. It also assumes that you`re sample is homogenous. 

--------------------------------CODE- START-----------------------------
iris = datasets.load_iris()
X = iris.data
variable_names = iris.feature_names

factor = FactorAnalysis().fit(X)

pd.DataFrame(factor.components_, columns=variable_names)
	sepal length	sepal width 	petal length	petal width
0	0.707227		-0.153147		1.653151		0.701569
1	0.114676		0.159763		-0.045604		-0.014052
2	-0.000000		0.000000		0.000000		0.000000
3	-0.000000		0.000000		0.000000		-0.000000
--------------------------------CODE - END------------------------------

----------------------------------------------------------------------------------------------------------------------	
PRINCIPAL COMPONENT ANALYSIS (PCA)

Singular Value Decomposition-(SVD) is a linear algebra method that you use to decompose a matrix into three resultant matrices. You do this in order to reduce information redundancy and noise. SVD is most commonly used for principle component analysis.

This is our original data set, it`s called A, and we decompose it into three resultant matrices, U, S and V. 
	U is the "left  orthogonal matrix", and it holds all of the important non-information-redundant information about the observations in the original data set. 
	V is the "right orthogonal matrix", and it holds all of the important non-redundant information on features in the original data set. 
	S, this is the "diagonal matrix", and it contains all of the information about the decomposition processes that were performed during the compression.

PCA USE-CASES :
	You can use PCA for 
		fraud detection, 
		spam detection, 
		image recognition, 
		speech recognition, or 
		outlier removal, if you are using PCA for data pre-processing. 

----------------------------CODE- START------------------------------------------
iris_pca = PCA().fit_transform(X)

pca.explained_variance_ratio_ #EXPLAINED VARIANCE RATION
array([0.92461621, 0.05301557, 0.01718514, 0.00518309])
"Look at the explained variance ratio. We see that the first component explains 92.4% of the data set`s variation. That means it holds 92.4% of the data`s information in one principal component. Pretty cool, right? And by taking the first two components, we only elude 2.3% of the data set`s information. That`s the junk we want to get rid of anyway, so let`s do that. We will take only the first two components and feel satisfied knowing that they contain 97.7% of the iris data set`s original information. "

pca.explained_variance_ratio_.sum() #CUMMULATIVE VARIANCE RATION
1.0

comps = pd.DataFrame(pca.components_, columns=variable_names)

	sepal length (cm)	sepal width (cm)	petal length (cm)	petal width (cm)
0	0.361590				-0.082269			0.856572			0.358844
1	0.656540				0.729712			-0.175767			-0.074706
2	-0.580997				0.596418			0.072524			0.549061
3	0.317255				-0.324094			-0.479719			0.751121
--------------------------CODE- END--------------------------------------------------


PCA and FACTOR Analysis Usage in MACHINE LEARNING
"YOU MAY BE WONDERING HOW YOU CAN USE FACTORS AND COMPONENTS. WELL, BOTH FACTORS AND COMPONENTS REPRESENT WHAT IS LEFT OF THE DATA SET AFTER INFORMATION REDUNDANCY AND NOISE HAVE BEEN STRIPPED OUT. YOU USE THEM AS INPUT VARIABLES FOR MACHINE LEARNING ALGORITHMS TO GENERATE PREDICTIONS FROM THESE COMPRESSED REPRESENTATIONS OF YOUR DATA."

 The results from this correlation heat map show that, 
 1. principal component one is strongly positively correlated with petal length and moderately positively correlated with sepal length and petal width. Component one is slightly negatively correlated with sepal width,  
 2. principal component number two is strongly negatively correlated with sepal length and sepal width, and slightly negatively correlated with petal length and petal width. You may be wondering how you can use these components once you`ve isolated. Well, you can use them as input variables for machine learning algorithms. So in the case of the iris data set, you could use the two components we`ve generated as input for a logistic regression model to predict species labels for new incoming data points. 

----------------------------------------------------------------------------------------------------------------------
QUIZ
1. How much of your data should go to the test set?
	one-third
2. The goal of PCA is to keep only the principal components that matter. 70%
************************************************************************************************************
************************************************************************************************************
5. OUTLIER ANALYSIS
************************************************************************************************************
EXTREME VALUE ANALYSIS USING UNIVARIATE METHODS
Types of Outliers
1. Point Outliers - Univariate Outliers
2. Contextual Outliers - Location,Temperature
3. Collective Outliers - These are a collection of observations that are anomalous but appear close to one another because they all have similar anomalous values. - DBSCAN
			(anomalous - deviating from what is standard, normal, or expected.)

Use of Outlier detection( Basically used to uncover anomalies in data)
	1. equipment failure notification, 
	2. fraud detection, and 
	3. cybersecurity event notification
----------------------------------------------------------------------------------------------------------------------
UNIVARIATE METHODS: 
	Tukey Boxlplots - You can detect unusually high or low data points in a variable by applying the Tukey methods for outlier detection
		Boxplot whiskers - IQR Inter Quantile Range
		"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch05/05_01/Univariate_Boxplot_Tukey.PNG"
df.boxplot(return_type=`dict`)
plt.plot()

def tukey_label(data):
    IQR = np.quantile(data, 0.75) - np.quantile(data, 0.25)
    Q1 = np.quantile(data, 0.25)
    Q3 = np.quantile(data, 0.75)
    #print(f"{IQR:.2f},{Q1:.2f},{Q3:.2f}")
    outlier_1 = Q1 - (IQR * 1.5)
    outlier_2 = Q3 + (IQR * 1.5)
    return (outlier_1,outlier_2)

Sepal_Width = X[:,1]
iris_outliers = (Sepal_Width > 4)
df[iris_outliers]

Sepal_Width = X[:,1]
iris_outliers = (Sepal_Width < 2.05)
df[iris_outliers]
"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch05/05_01/05_01.ipynb"


----------------------------------------------------------------------------------------------------------------------
MULTIVARIATE ANALYSIS FOR OUTLIER DETECTION
Box-plot
df.columns=[`Sepal Length`,`Sepal Width`,`Petal Length`,`Petal Width`, `Species`]
data = df.iloc[:,0:4].values
target = df.iloc[:,4].values

sb.boxplot(x=`Species`, y=`Sepal Length`, data=df, palette=`hls`)

pair-plot
sb.pairplot(df, hue=`Species`, palette=`hls`)


----------------------------------------------------------------------------------------------------------------------
A LINEAR PROJECTION METHOD FOR MULTIVARIATE DATA
from sklearn.cluster import DBSCAN
model = DBSCAN(eps=0.8, min_samples=19).fit(data)
print(model)
>>> DBSCAN(algorithm=`auto`, eps=0.8, leaf_size=30, metric=`euclidean`,
    metric_params=None, min_samples=19, n_jobs=1, p=None)

outliers_df = pd.DataFrame(data)
print(model.labels_)
>>>
[ 0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0
  0  0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1 -1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1 -1 -1  1
  1  1 -1  1  1  1  1  1  1  1  1 -1  1  1  1  1  1  1  1  1  1  1  1  1
  1  1  1  1  1  1]


from collections import Counter
print(Counter(model.labels_))
>>>Counter({1: 94, 0: 50, -1: 6})

#Predicted Outliers
print(outliers_df[model.labels_ ==-1])
       0    1    2    3
98   5.1  2.5  3.0  1.1
105  7.6  3.0  6.6  2.1
117  7.7  3.8  6.7  2.2
118  7.7  2.6  6.9  2.3
122  7.7  2.8  6.7  2.0
131  7.9  3.8  6.4  2.0

#Plotting DBSCAN Predictions using Scatter plot

fig = plt.figure()
ax = fig.add_axes([.1, .1, 1, 1]) 

colors = model.labels_

ax.scatter(data[:,2], data[:,1], c=colors, s=120)#Petal Length , Sepal width
ax.set_xlabel(`Petal Length`)
ax.set_ylabel(`Sepal Width`)
plt.title(`DBScan for Outlier Detection`)

DBSCAN Assumptions:
Just make sure that the number of outliers you choose is less than 5% of the total number of observations in your dataset.You do that by adjusting your model parameters accordingly. 

DBSCAN Parameters:
eps and min_samples. 
	"Eps" sets the maximum distance between two samples for them to be clustered in the same neighborhood. You want to start with an eps value of zero point one. 
	As far as "min_samples", this is the minimum number of samples in a neighborhood for a data point to qualify as a core point. Again, here, you want to start with a very low sample size. You adjust your parameters until you`ve got just less than 5% of your total dataset size, labeled as outliers.

For Example: 
model = DBSCAN(eps=0.8, min_samples=19).fit(data)
and
model = DBSCAN(eps=1, min_samples=30).fit(data)
Produces 6 Outliers which is less that 5%(i.e 7.5) of total sample size i.e 150


----------------------------------------------------------------------------------------------------------------------
QUIZ
1. Using Tukey outlier labeling, the lower quartile ends at 3.6 and the upper quartile starts at 4.8. Which of the following is not an outlier?
6.5
	IQR = 4.8 - 3.6 = 1.2
    Q1 = 3.6
    Q3 = 4.8
    #print(f"{IQR:.2f},{Q1:.2f},{Q3:.2f}")
    outlier_1 = Q1 - (IQR * 1.5) = 3.6 - (1.2*1.5) = 1.8  # lesser that 1.8 is a outlier
    outlier_2 = Q3 + (IQR * 1.5)  = 4.8 + (1.2*1.5) = 6.6‬ # Greater that 6.6‬ is a outlier
************************************************************************************************************
6. CLUSTER ANALYSIS
************************************************************************************************************
K-MEANS METHOD - \UNSUPERVISED
Used to predict Subgroups from withiin a dataset.
Predictions are based on : 
	1. Number of cluster centers that are present,T
	2. The nearest mean values between observations.(Measured using euclidean distance between observations)
Use-Cases : 
	1. Market price and cost modeling, 
	2. Customer segmentation, 
	3. Hedge fund classification, 
	4. Insurance claim fraud detection.
Assumptions : 
	1. the first thing is you always need to "scale your variables" before clustering your data,  - scale(iris.data) 
	2. you need to look at a scatter plot or a data table to estimate the "number of cluster centers" to set for the k parameter in the model. n_clusters=3
Evaluation : 
	classification_report(y, relabel)

------------------------CODE START------------------------
from sklearn.cluster import KMeans
clustering = KMeans(n_clusters=3, random_state=5)
clustering.fit(X)
y_pred = clustering.labels_

Plotting K-MEANS
color_theme = np.array([`darkgray`, `lightsalmon`, `powderblue`])

plt.subplot(1,2,1)
plt.scatter(x=iris_df.Petal_Length,y=iris_df.Petal_Width, c=color_theme[iris.target], s=50)
plt.title(`Ground Truth Classification`)

plt.subplot(1,2,2)
plt.scatter(x=iris_df.Petal_Length,y=iris_df.Petal_Width, c=color_theme[clustering.labels_], s=50)
plt.title(`K-Means Classification`)

Evaluating K-MEANS
print(classification_report(y, relabel))
------------------------CODE END------------------------

----------------------------------------------------------------------------------------------------------------------
HIERARCHICAL METHODS - \UNSUPERVISED
Hierarchical clustering is an unsupervised machine learning method that you can use to predict subgroups based on 
	1. the "Distance" between data points and their nearest neighbors.
	2. Each data point is "linked" to its neighbor that is most nearby according to the distance metric that you choose.
(Hierarchical clustering predicts subgroups within data by finding the distance between each data point and its nearest neighbor and also linking up the most nearby neighbors.)
DENDROGRAM
	You can find the number of subgroups(like number or cluster centers in K-Means) that are appropriate for a hierarchical clustering model by looking at a dendrogram. 
	A dendrogram is a tree graph that`s useful for visually displaying taxonomies, lineages, and relatedness. 
	"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch06/06_02/Hierarchical_Clustering_Dendogram.PNG"
Use-Cases : 
	1. hospital resource management, 
	2. business process management, 
	3. customer segmentation analysis, 
	4. social network analysis. 
Assumptions : 
	1. Number of Centroids - ( Using dendrogram)
	2. Maximum distance between points = 150 for example.
Dendrogram Use example: 
	Imagine that you only want a maximum distance between a point and its nearest neighbor. You want that maximum distance to be 150. You would plot out your dendrogram, and then you would set a line where y=150. Then along that line, you look at each point where your dendrogram intersects the y=150 line, so you have one, two, three, four, five. If you wanted to have a maximum distance of 150 between each point and its nearest neighbor, you would have five cluster centers. You could also set the maximum distance between points to be 500, an in that case you would have two cluster centers.
	"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch06/06_02/Hierarchical_Clustering_Dendogram_Example.PNG"
Parameter : 
	1. Distance Metrics  : Distance metrics can be set as either Euclidean, Manhattan, or Cosine
	2. Linkage 			 : The linkage parameters are Ward, Complete, and Average. 
	(Use trial and error )

------------------------CODE START------------------------
from scipy.cluster.hierarchy import dendrogram, linkage
Z = linkage(X, `ward`)
# Z is the clustering results that have been generated from the Scipy hierarchical clustering algorithm. 
dendrogram(Z, truncate_mode=`lastp`, p=12, leaf_rotation=45., leaf_font_size=15., show_contracted=True)

plt.title(`Truncated Hierarchical Clustering Dendrogram`)
plt.xlabel(`Cluster Size`)
plt.ylabel(`Distance`)


plt.axhline(y=500) # if max distance = 500 then Clusters = 2
plt.axhline(y=150) # if max distance = 150 then Clusters = 5
plt.show()


#  With 500 set as our maximum distance between nearest neighbors, based on the dendrogram, we have two clusters. 
# Also, the Target variable AM feature assums two possible outcomes, 0 or 1 so k =2 
k=2

Hclustering = AgglomerativeClustering(n_clusters=k, affinity=`euclidean`, linkage=`ward`)
Hclustering.fit(X)

sm.accuracy_score(y, Hclustering.labels_)
>>>0.78125

					@NOTE - "Ward" can only work with "euclidean" distances. 
					Results of 
						ecludiean and Ward, average,Complete
						manhattan and 		average,Complete
						cosine    and 		average,Complete

------------------------CODE END------------------------

----------------------------------------------------------------------------------------------------------------------
INSTANCE-BASED LEARNING WITH K-NEAREST NEIGHBOR \SUPERVISED
K-nearest neighbor works by memorizing observations within a labeled Training set to predict classification labels for new, incoming, unlabeled observations. The algorithm makes predictions based on how similar training observations are to the new, incoming observations. The more similar the observation`s value, the more likely they will be classified with the same label.

Use-Cases : 
	1. stock price prediction, 
	2. recommendation systems, 
	3. predictive trip planning, 
	4. credit risk analysis. 

Assumptions : 
	1. the data set has little noise, 
	2. it`s labeled, 
	3. it contains only relevant features, 
	4. the data set has distinguishable sub groups.
	5. Small or medium sized Data sets

**NOTE**
	Takes too Long to classify for large datasets so avoid for that case.



------------------------CODE START------------------------
#Modelling of KNN
clf = neighbors.KNeighborsClassifier()
clf.fit(X_train, y_train)
print(clf)
	>>> KNeighborsClassifier(algorithm=`auto`, leaf_size=30, metric=`minkowski`,
           metric_params=None, n_jobs=1, n_neighbors=5, p=2,
           weights=`uniform`)

#Prediction of KNN
y_pred = clf.predict(X_test)

#Evaluation of KNN
metrics.classification_report(y_test, y_pred)
>>>
	             precision    recall  f1-score   support

          0       0.71      1.00      0.83         5
          1       1.00      0.67      0.80         6

avg / total       0.87      0.82      0.82        11


sm.accuracy_score(y_expect, y_pred)
>>>	0.84375

#Plotting KNN Classification
color_theme = np.array([`red`, `blue`])

plt.subplot(1,2,1)
plt.scatter(x=X[:,0],y=X[:,1], c=color_theme[y], s=50)
plt.title(`Ground Truth Classification`)

plt.subplot(1,2,2)
plt.scatter(x=X[:,0],y=X[:,1], c=color_theme[y_], s=50)
plt.title(`K-Nearest Neighbor`)

------------------------CODE END------------------------


QUIZ
1. The classification_report() function can be used to evaluate the accuracy of the K-means clustering analysis.
2. FALSE  - The k-NN model is especially useful for very large datasets. (Takes too long for Large datasets, so avoid.)



************************************************************************************************************
7. NETWORK ANALYSIS WITH NETWORKX
************************************************************************************************************
INTRO TO NETWORK ANALYSIS
Uses of Network Analysis
	1. Social Media Marketing Strategy
	2. Infrastructure system design, 
	3. Financial risk management, 
	4. Public health management.

Difference Network and Graph
	Network 
		A body of connected data that is evaluated during graph analysis 
	Graph 
		The actually data "visualization" schematic that depicts the network data. 
"Nodes" are vertices around which the graph is formed and 
"Edges" are the lines that connect them.
"Directed graph", also known as a digraph, is a graph where there is a direction assigned to each edge that connects the nodes. 
"Directed edge" is an edge feature that has been a assigned direction between those nodes.
"Undirected graphs" - graphs where all the edges are bidirectional
"Undirected edge" is just an edge that flows both ways between nodes.
"Graph size" is the number of edges in a graph. 
"Graph order" is the number of vertices in a graph
"Degree" refers to the number of edges connected to a vertex. It`s a measure of connectedness. 

Graph Generators
One important applications of graph generators, is for generating synthetic variations of a particular graph, so that you can use those synthetic graphs to test and validate how well your graph assessment algorithm performs.
Basically, use graph assessment algorithms to extract a meaningful insight from network data. The structure of your underlying data and your analytical objectives would determine which type of graph assessment algorithm is appropriate, but as for the graph generators, there are only six types. 

Types of Graph Generators
	1. Graph drawing algorithms, ✔
	2. Network analysis algorithms, 
	3. Algorithmic routing for graphs, 
	4. Graph search algorithms, ✔ 
	5. Subgraph algorithms.



----------------------------------------------------------------------------------------------------------------------
WORK WITH GRAPH OBJECTS


------------------------CODE START------------------------
"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch07/07_02/07_02.ipynb"
G = nx.Graph()
nx.draw(G)

G.add_node(1)
nx.draw(G)

G.add_nodes_from([2,3,4,5,6,8,9,12,15,16])
nx.draw(G)

G.add_edges_from([(2,4),(2,6),(2,8),(2,12),(2,16),(3,6),(3,9), (3,12),(3,15),(4,8),(4,12),(4,16),(6,12),(8,16)])
nx.draw(G)

#Different shapes of same graph Spring, Circular etc/=.
#The basics about drawing graph objects
nx.draw_circular(G)
nx.draw_spring(G)

#Labeling and coloring your graph plots
nx.draw_circular(G, node_color=`bisque`, with_labels=True)

#Remove a node
G.remove_node(1)
nx.draw_circular(G, node_color=`bisque`, with_labels=True)

#Identifying graph properties / stats / info
sum_stats = nx.info(G)
print(sum_stats)
>>>
		Name: 
		Type: Graph
		Number of nodes: 10
		Number of edges: 14
		Average degree:   2.8000
		 "Average degree sugggests that on average 2.8 nodes are connected to each node."

print(nx.degree(G))
>>> [(2, 5), (3, 4), (4, 4), (5, 0), (6, 3), (8, 3), (9, 1), (12, 4), (15, 1), (16, 3)]
 "This sugggests that the node 2 is connected to 5 nodes, node 3 is connected to 4 ...  and node 16 is connected to 3 nodes. Summing them all you get 28 and total number of nodes is 10. Therefore avg degree is 28/10 = 2.8"

#Using graph generators to generated grapgh automatically unlike the above manual method - complete_graph
G = nx.complete_graph(25) # 25 = no of nodes
nx.draw(G, node_color=`bisque`, with_labels=True)

# gnc_graph - To Draw Direccted Random Grapgh
G = nx.gnc_graph(7, seed=25)
nx.draw(G, node_color=`bisque`, with_labels=True)

# ego_graph - a sub-graph generator.
"Social networks are an excellent real life example of the ego network. "
ego_G = nx.ego_graph(G, 3, radius=5)
nx.draw(G, node_color=`bisque`, with_labels=True)
" So this is a similar structure as you would find with a social network, and you`ll notice here how it`s directional. So in this case, if this was a Twitter network you would say that the person at node five is following the person at node four and at node zero. "
------------------------CODE END------------------------

Visualization types
Normal, 	Cricular, 			Spring, 	     Spectral,
draw(G), draw_circular(G), draw_spring(G) , draw_spectral(G) 
----------------------------------------------------------------------------------------------------------------------
SIMULATE A SOCIAL NETWORK
To simulate a social network in three easy steps. 
	1: to generate a graph object and edgelist,  (gn_graph, generate_edgelist)
	2: assign attributes to graph nodes, 
	3: visualize the network. 
------------------------CODE START------------------------
"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch07/07_03/07_03.ipynb"
#1: to generate a graph object and edgelist, 
DG = nx.gn_graph(7, seed=25)
for line in nx.generate_edgelist(DG, data=False): print(line)
>>> 	1 0
		2 0
		3 2
		4 3
		5 0
		6 4

#2: assign attributes to graph nodes
DG.node[0][`name`] = `Alice`  #{`name`: `Alice`}
DG.node[1][`name`] = `Bob`
DG.node[2][`name`] = `Claire`
DG.node[3][`name`] = `Dennis`
DG.node[4][`name`] = `Esther`
DG.node[5][`name`] = `Frank`
DG.node[6][`name`] = `George`

DG.add_nodes_from([(0,{`age`:25}),(1,{`age`:31}),(2,{`age`:18}),(3,{`age`:47}),(4,{`age`:22}),
                   (5,{`age`:23}),(6,{`age`:50})])
>>>{`name`: `Alice`, `age`: 25}

DG.node[0][`gender`] = `f`
DG.node[1][`gender`] = `m`
DG.node[2][`gender`] = `f`
DG.node[3][`gender`] = `m`
DG.node[4][`gender`] = `f`
DG.node[5][`gender`] = `m`
DG.node[6][`gender`] = `m`

#3: visualize the network. 
nx.draw_circular(DG, node_color=`bisque`, with_labels=True)

labeldict = {0: `Alice`,1:`Bob`,2:`Claire`,3:`Dennis`,4:`Esther`,5:`Frank`,6:`George`}
nx.draw_circular(DG, labels=labeldict, node_color=`bisque`, with_labels=True)


G = DG.to_undirected()
nx.draw_spectral(G, labels=labeldict, node_color=`bisque`, with_labels=True)
------------------------CODE END------------------------


----------------------------------------------------------------------------------------------------------------------
GENERATE STATS ON NODES AND INSPECT GRAPHS
Metrics that network analysis experts use to make quick sense of who`s who in a social media network.

Degree 		: Describes a node`s connectedness. 
Successors 	: A successor node is a node that could serve as a backup and potentially a replacement for an influential node in a network. The function of a successor node is to preserve the flow of influence throughout a network, in the case where an important node is removed.
Neighbors 	: Refers to adjacent nodes in a network. 
in-degree	: Refers to a theory that if a person or profile has a lot of inbound connections, then they`re considered prestigious because many different people and profiles want to connect with them. 
out-degree	: This term refers to when a person or profile has a lot of outgoing connections. These people are sometimes also considered influential because they have a large network, across which they can engage and interact with their many outbound connections. 
----------------------------------------------------------------------------------------------------------------------


"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch07/07_04/07_04.ipynb"


QUIZ:
1. A graph has 4 nodes and 5 lines that connect them. This graph therefore has a size of 5 and an order of 4 .
2. What function would you use to generate a graph that represents a social network?
	ego_graph()
3. In a social network, a certain node has the most connections. Therefore, this node is influential or prestigious .





************************************************************************************************************
8. BASIC ALGORITHMIC LEARNING
************************************************************************************************************
LINEAR REGRESSION MODEL
Linear regression is a simple machine learning method that you can use to predict an observation`s value based on the relationship between the target variable and independent, linearily related numeric predictive features. 
FOR EXAMPLE, imagine you have a dataset that describes key characteristics of a "SET OF HOMES". Like land acreage, number of stories, building area and sales price. Based on these features, and the relationship with the sales price of these homes, you could build a multi variate linear model that predicts the price a house can be sold for based on it`s features. 
Linear regression is a statistical machine learning method, and there are two different versions, there`s simple linear regression, where you have one predictor and one predictant. And multiple linear regression, that`s where you have multiple predictors and one predictant.
Use-Cases :
	1. sales forecasting, 
	2. resource consumption forecasting, 
	3. supply cost forecasting,
	4. telecom services lifecycle forecasting. 

Assumptions :
 	1. All variables are continuous numeric variables and not categorical ones,
 	2. Your data is free of missing values and outliers.
 	3. There`s a linear relationship between predictors and predictant, 
 	4. "ALL PREDICTORS ARE INDEPENDENT OF ONE ANOTHER,"
 	5. Your residuals are normally distributed. 

In Our example 1. and 2. are satisfied
For 3. Linear relationship between target-(Roll) and predictor - (year, h-grad , unem, inc) is as follows
			    roll    
		year  0.900934  
		roll  1.000000  
		unem  0.391344  
		hgrad 0.890294  
		inc   0.949876  
 	So, year is not a continous variable rather a rank or index
 	hgrad and inc has strong correlation with roll.
 	unem has relatively low correlation

Although, 4. suggests to check inter-correlation since predictors should be independent. 
			     unem     hgrad       inc
		unem   1.000000  0.177376  0.282310
		hgrad  0.177376  1.000000  0.820089
		inc    0.282310  0.820089  1.000000
	So hgrad and inc are no independent and thus cannot be used together to predict roll
	Instead, unem and higrad are strong independent, thus we can use them as predictors.
Also can be checked using regplot()
	sns.regplot(x='inc', y='hgrad', data=enroll, scatter=True)
	sns.regplot(x='unem', y='hgrad', data=enroll, scatter=True)
	sns.regplot(x='unem', y='inc', data=enroll, scatter=True)

#predictors = unem and higrad
print(LinReg.score(X,y))  
>>> 84.88%

#predictors = unem and inc
print(LinReg.score(X,y))  
>>> 91.87%

"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch08/08_01/08_01.ipynb"

----------------------------------------------------------------------------------------------------------------------
LOGISTIC REGRESSION MODEL
Logistic regression is a simple machine learning method that you can use to predict an observation`s category based on the relationship between the target feature and independent categorical predictive features in the data set. 

For example, imagine you`re a marketing data scientist for a major telecom service provider. You`ve got a customer data set that describes each customer with variables like age, income, average call duration, interaction history with customer support, leftover minutes per month and customer status. Customer status is a variable that describes whether a customer is active or has canceled services. Based on the predictive features in this data set, in their relationship with a customer status variable, you could build a logistic regression model that predicts whether a customer is likely to cancel services in the near future. This is called a customer churn model. "CUSTOMER CHURN MODEL"

Linear vs Logistic
Logistic regression differs from linear regression in that, with logistic regression, you`re predicting categories for ordinal variables, in linear progression you`re predicting values for numeric continuous variables. 

Use-Cases 	:
	1. purchase propensity versus ad spend analysis, 
	2. customer churn prediction, 
	3. employee attrition modeling 
	4. hazardous event prediction. 

Assumptions :
	Logistic regression has a lot less assumptions than linear regression, but there are some. 
	1. data is free of missing values. 
	2. the predictant variable is binary, in other words it only accepts two values, or it could be ordinal, a categorical variable with ordered values. 
	3. All predictors are independent of each other. 
	4. There are at least 50 observations per predictor variable to ensure reliable results.


#Checking for independence between features
#spearmanr because drat carb are ordinal datas, i.e. numerical but can be converted to categorical
spearmanr_coefficient, p_value =  spearmanr(drat, carb)
print("Spearman Rank Correlation Coefficient %0.3f" % (spearmanr_coefficient)) 
>>>-0.125 #So No Relation

sb.regplot(x='drat', y='carb', data=cars, scatter=True)
#regplot() And we can see that these are categorical values. Neither of these variables take on an infinite number of positions, they only take on set positions.

# Checking for missing values
cars.isnull().sum()

# Checking that your target is binary or ordinal
print(cars.am.value_counts())
sb.countplot(x='am', data=cars, palette='hls')

cars.info()
----------------------------------------------------------------------------------------------------------------------
NAÏVE BAYES CLASSIFIERS
----------------------------------------------------------------------------------------------------------------------
Naive Bayes classification is a machine learning method that you can use to predict the likelihood that an event will occur given evidence that`s supported in a dataset. 
For the demo in this segment, we`re going to build a Naive Bayes classifier from our large dataset of emails called spam base. Some of the records in the dataset are marked as spam and all of the other records are marked as not spam. The predictive features in this dataset serve as our evidence. Using them, we can build a "SPAM FILTERING SYSTEM" with a Naive Bayes model and successfully predict which incoming emails are spam and which are not. 
Naive Bayes is a machine learning method that you can use to predict the likelihood that an event will occur given evidence that`s present in your data. This is also called CONDITIONAL PROBABILITY in the world of statistics.
Types of Naive Bayes models. 
	1. Multinomial, 
	2. Bernoulli, and 
	3. Gaussian. 

Multinomial is good for when your features are categorical or continuous and describe discrete frequency counts. In other words, word counts or something like that. 

Bernoulli is good for making predictions from binary features. 

Gaussian approach is good for making predictions from normally distributed features.

Use-Cases 	: 
	1. spam detection, 
	2. customer classification, 
	3. credit risk prediction,
	4. health risk prediction. 

Assumptions  :
	1. Predictors are independent of one another 
	2. Predictors also has an a priori assumption. 
		This assumption is that the past condition still hold true, meaning that when we make predictions from historical values, we`ll get incorrect results if present circumstances change. I just want to point out that all regression models including the linear regression model and logistic regression we just covered, they all maintain a priori assumption.



\ACCURACY  	= 	tp 	+  tn / (Total) 
accuracy_score()

------------------------CODE START------------------------

from sklearn.naive_bayes import BernoulliNB
from sklearn.naive_bayes import GaussianNB
from sklearn.naive_bayes import MultinomialNB


1.
BernNB = BernoulliNB(binarize=True)
BernNB.fit(X_train, y_train)
print(BernNB)

y_expect = y_test
y_pred = BernNB.predict(X_test)
print( accuracy_score(y_expect, y_pred))
>>>BernoulliNB(alpha=1.0, binarize=True, class_prior=None, fit_prior=True)
	0.8558262014483212


2. 
MultiNB = MultinomialNB()

MultiNB.fit(X_train, y_train)
print(MultiNB)

y_pred = MultiNB.predict(X_test)
print( accuracy_score(y_expect, y_pred))
>>>MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
	0.8736010533245556


3. 
GausNB = GaussianNB()
GausNB.fit(X_train, y_train)
print(GausNB)

y_pred = GausNB.predict(X_test)
print( accuracy_score(y_expect, y_pred))
>>>GaussianNB(priors=None)
	0.8130348913759052


4. BernNB = BernoulliNB(binarize=0.1)
BernNB.fit(X_train, y_train)
print(BernNB)

y_expect = y_test
y_pred = BernNB.predict(X_test)
print( accuracy_score(y_expect, y_pred))
>>> BernoulliNB(alpha=1.0, binarize=0.1, class_prior=None, fit_prior=True)
	0.8953258722843976

------------------------CODE END------------------------

QUIZ
1. Before running a linear regression, you should confirm that there are no missing values in the data.
2. Data that has binary features is best predicted using the "Bernoulli" Naïve Bayes model.


************************************************************************************************************
9. WEB-BASED DATA VISUALIZATIONS WITH PLOTLY
************************************************************************************************************
CREATE BASIC CHARTS
----------------------------------------------------------------------------------------------------------------------
CREATE STATISTICAL CHARTS
----------------------------------------------------------------------------------------------------------------------
CREATE PLOTLY CHOROPLETH MAPS
----------------------------------------------------------------------------------------------------------------------
CREATE PLOTLY POINT MAPS
fig = dict(data=data, layout=layout)

py.iplot(fig, filename='d3-choropleth-map')
----------------------------------------------------------------------------------------------------------------------

QUIZ
1. In a Jupyter notebook, can Plotly charts be created directly from Pandas DataFrames?
	Yes, by installing the Cufflinks library and calling the iplot() method from the DataFrame object.
2. For the iplot() method to plot three histograms stacked vertically, specify the parameter(s)
	kind='histogram'
	subplots=True
	shape=(3,1)
3. When building a choropleth data object for Plotly, which dictionary parameter specifies the country to map?
	locationmode
************************************************************************************************************
10. WEB SCRAPING WITH BEAUTIFUL SOUP
************************************************************************************************************
"https://github.com/Gurubux/Data-Lit/blob/4ec3099d08941d8b9b71361cbcdff2a65093af5e/1-DataCollection/1.4_Cleaning_Data/Webscraping_MoviePostersFromImdb.ipynb"
- Objects in BeautifulSoup, and how to work with them. 
- How to work with parse data, scrape a web page, and save your results. 

INTRODUCTION TO BEAUTIFUL SOUP
Web-Scraping Use Cases :
	1. Links from Existing blog to a New blog`s resource page.
	2. Ecommerce store automation, 
	3. Hydrological analysis, 
	4. Emergency resource allocation, 
	5. Oil and gas production intel.

Objects in BeautifulSoup 4:
	1. BeautifulSoup object,
	2. Tag object, 
	3. NavigatableString object, 
	4. Comment object.

------------------------CODE START------------------------
from bs4 import BeautifulSoup

#The BeautifulSoup object
soup = BeautifulSoup(html_doc, 'html.parser') #html_doc = string object with <html> code
<html><head><title>Best Books</title></head>
<body>
<p class="title"><b>DATA SCIENCE FOR DUMMIES</b></p>
<p class="description">Jobs in data science abound, but few people have the data science skills needed to fill these increasingly important roles in organizations. Data Science For Dummies is the pe
<br/><br/>
Edition 1 of this book:
        <br/>
<ul>
<li>Provides a background in data science fundamentals before moving on to working with relational databases and unstructured data and preparing your data for analysis</li>
<li>Details different data visualization techniques that can be used to showcase and summarize your data</li>
<li>Explains both supervised and unsupervised machine learning, including regression, model validation, and clustering techniques</li>
<li>Includes coverage of big data processing tools like MapReduce, Hadoop, Storm, and Spark</li>
</ul>
<br/><br/>
What to do next:
<br/>
<a class="preview" href="http://www.data-mania.com/blog/books-by-lillian-pierson/" id="link 1">See a preview of the book</a>,
<a class="preview" href="http://www.data-mania.com/blog/data-science-for-dummies-answers-what-is-data-science/" id="link 2">get the free pdf download,</a> and then
<a class="preview" href="http://bit.ly/Data-Science-For-Dummies" id="link 3">buy the book!</a>
</p>
<p class="description">...</p>
</body></html>

print(soup.prettify()[0:350])
<html>
 <head>
  <title>
   Best Books
  </title>
 </head>
 <body>
  <p class="title">
   <b>
    DATA SCIENCE FOR DUMMIES
   </b>
  </p>
  <p class="description">
   Jobs in data science abound, but few people have the data science skills needed to fill these increasingly important roles in organizations. Data Science For Dummies is the pe
   <br/

#Tag objects
#Working with names
soup = BeautifulSoup('<b body="description"">Product Description</b>', 'html')
tag=soup.b
type(tag)
>>>bs4.element.Tag

tag
>>><b body="description">Product Description</b>

tag.name
'b'

tag.name = 'bestbooks'
tag
<bestbooks body="description">Product Description</bestbooks>

tag.name
'bestbooks'

#Working with attributes
tag['body']
>>>'description'
tag.attrs
>>>  {'body': 'description'}
tag['id'] = 3
tag.attrs
>>> {'body': 'description', 'id': 3}
tag
>>>
<bestbooks body="description" id="3">Product Description</bestbooks>
del tag['body']
del tag['id']
tag
>>>
<bestbooks>Product Description</bestbooks>
tag.attrs


#Using tags to navigate a tree
soup.head
soup.title
soup.body.b
soup.body
soup.ul
soup.a


------------------------CODE END------------------------

----------------------------------------------------------------------------------------------------------------------
EXPLORE NAVIGATABLESTRING OBJECTS
"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch10/10_02/10_02.ipynb"
----------------------------------------------------------------------------------------------------------------------
PARSE DATA
text_only = soup.get_text()
print(text_only)
>>>
Best Books

DATA SCIENCE FOR DUMMIES
Jobs in data science abound, but few people have the data science skills needed to fill these increasingly important roles in organizations. Data Science For Dummies is the pe

Edition 1 of this book:
        

Provides a background in data science fundamentals before moving on to working with relational databases and unstructured data and preparing your data for analysis
Details different data visualization techniques that can be used to showcase and summarize your data
Explains both supervised and unsupervised machine learning, including regression, model validation, and clustering techniques
Includes coverage of big data processing tools like MapReduce, Hadoop, Storm, and Spark


What to do next:

See a preview of the book,
get the free pdf download, and then
buy the book!
...

Retrieving tags by filtering with name arguments
soup.find_all("li")
1
soup.find_all("li")
[<li>Provides a background in data science fundamentals before moving on to working with relational databases and unstructured data and preparing your data for analysis</li>,
 <li>Details different data visualization techniques that can be used to showcase and summarize your data</li>,
 <li>Explains both supervised and unsupervised machine learning, including regression, model validation, and clustering techniques</li>,
 <li>Includes coverage of big data processing tools like MapReduce, Hadoop, Storm, and Spark</li>]
Retrieving tags by filtering with keyword arguments
1
soup.find_all(id="link 3")
[<a class="preview" href="http://bit.ly/Data-Science-For-Dummies" id="link 3">buy the book!</a>]
Retrieving tags by filtering with string arguments
1
soup.find_all('ul')
[<ul>\n<li>Provides a background in data science fundamentals before moving on to working with relational databases and unstructured data and preparing your data for analysis</li>\n<li>Details different data visualization techniques that can be used to showcase and summarize your data</li>\n<li>Explains both supervised and unsupervised machine learning, including regression, model validation, and clustering techniques</li>\n<li>Includes coverage of big data processing tools like MapReduce, Hadoop, Storm, and Spark</li>\n</ul>]
Retrieving tags by filtering with list objects
1
soup.find_all(['ul', 'b'])
[<b>DATA SCIENCE FOR DUMMIES</b>,
 <ul>\n<li>Provides a background in data science fundamentals before moving on to working with relational databases and unstructured data and preparing your data for analysis</li>\n<li>Details different data visualization techniques that can be used to showcase and summarize your data</li>\n<li>Explains both supervised and unsupervised machine learning, including regression, model validation, and clustering techniques</li>\n<li>Includes coverage of big data processing tools like MapReduce, Hadoop, Storm, and Spark</li>\n</ul>]
Retrieving tags by filtering with regular expressions
1
l = re.compile('l')
2
for tag in soup.find_all(l): print(tag.name)
html
title
ul
li
li
li
li
Retrieving tags by filtering with a Boolean value
1
for tag in soup.find_all(True): print(tag.name)
html
head
title
body
p
b
p
br
br
br
ul
li
li
li
li
br
br
br
a
a
a
p
Retrieving weblinks by filtering with string objects
1
for link in soup.find_all('a'): print(link.get('href'))
http://www.data-mania.com/blog/books-by-lillian-pierson/
http://www.data-mania.com/blog/data-science-for-dummies-answers-what-is-data-science/
http://bit.ly/Data-Science-For-Dummies
Retrieving strings by filtering with regular expressions
1
soup.find_all(string=re.compile("data"))
[u'Jobs in data science abound, but few people have the data science skills needed to fill these increasingly important roles in organizations. Data Science For Dummies is the pe\n',
 u'Provides a background in data science fundamentals before moving on to working with relational databases and unstructured data and preparing your data for analysis',
 u'Details different data visualization techniques that can be used to showcase and summarize your data',
 u'Includes coverage of big data processing tools like MapReduce, Hadoop, Storm, and Spark']

----------------------------------------------------------------------------------------------------------------------
WEB SCRAPE IN PRACTICE
----------------------------------------------------------------------------------------------------------------------
Scraping a webpage and saving your results
r = urllib.request.urlopen('https://analytics.usa.gov').read()
soup = BeautifulSoup(r, "lxml")
type(soup)

print( soup.prettify()[:100])

for link in soup.find_all('a'): print(link.get('href'))


for link in soup.findAll('a', attrs={'href': re.compile("^http")}): print( link)



file = open('parsed_data.txt', 'w')
for link in soup.findAll('a', attrs={'href': re.compile("^http")}):
    soup_link = str(link)
    print( soup_link)
    file.write(soup_link)
file.flush()
file.close()




QUIZ
1. What will you get when calling tag.attrs on the following tag? 
 <a href='map.html' id='link'>Open Map</a>
		{'href' : 'map.html', 'id' : 'link'}

2. For the tag <b>Cancel</b> tag.string.parent simply returns the tag itself.
3. Calling the find_all() method on a BeautifulSoup object with a Boolean False as the parameter will retrieve no tags at all .
4. FALSE  : When scraping the web, it is always more useful to print the results inside the Jupyter notebook rather than saving them to a text file.
************************************************************************************************************
CONCLUSION
************************************************************************************************************