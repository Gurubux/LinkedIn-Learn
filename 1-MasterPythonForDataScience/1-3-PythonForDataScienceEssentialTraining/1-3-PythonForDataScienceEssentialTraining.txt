https://www.linkedin.com/learning/python-for-data-science-essential-training
INTRODUCTION
WELCOME
WHAT YOU SHOULD KNOW
GETTING STARTED WITH JUPYTER

1. DATA MUNGING BASICS
FILTER AND SELECT DATA
TREAT MISSING VALUES
REMOVE DUPLICATES
CONCATENATE AND TRANSFORM DATA
GROUP AND AGGREGATE DATA

2. DATA VISUALIZATION BASICS
CREATE STANDARD LINE, BAR, AND PIE PLOTS
DEFINE PLOT ELEMENTS
FORMAT PLOTS
CREATE LABELS AND ANNOTATIONS
CREATE VISUALIZATIONS FROM TIME SERIES DATA
CONSTRUCT HISTOGRAMS, BOX PLOTS, AND SCATTER PLOTS

3. BASIC MATH AND STATISTICS
USE NUMPY ARITHMETIC
GENERATE SUMMARY STATISTICS
SUMMARIZE CATEGORICAL DATA
PARAMETRIC METHODS
NON-PARAMETRIC METHODS
TRANSFORM DATASET DISTRIBUTIONS

4. DIMENSIONALITY REDUCTION
INTRODUCTION TO MACHINE LEARNING
EXPLANATORY FACTOR ANALYSIS
PRINCIPAL COMPONENT ANALYSIS (PCA)

5. OUTLIER ANALYSIS
EXTREME VALUE ANALYSIS USING UNIVARIATE METHODS
MULTIVARIATE ANALYSIS FOR OUTLIER DETECTION
A LINEAR PROJECTION METHOD FOR MULTIVARIATE DATA

6. CLUSTER ANALYSIS
K-MEANS METHOD
HIERARCHICAL METHODS
INSTANCE-BASED LEARNING WITH K-NEAREST NEIGHBOR

7. NETWORK ANALYSIS WITH NETWORKX
INTRO TO NETWORK ANALYSIS
WORK WITH GRAPH OBJECTS
SIMULATE A SOCIAL NETWORK
GENERATE STATS ON NODES AND INSPECT GRAPHS

8. BASIC ALGORITHMIC LEARNING
LINEAR REGRESSION MODEL
LOGISTIC REGRESSION MODEL
NAÏVE BAYES CLASSIFIERS

9. WEB-BASED DATA VISUALIZATIONS WITH PLOTLY
CREATE BASIC CHARTS
CREATE STATISTICAL CHARTS
CREATE PLOTLY CHOROPLETH MAPS
CREATE PLOTLY POINT MAPS

10. WEB SCRAPING WITH BEAUTIFUL SOUP
INTRODUCTION TO BEAUTIFUL SOUP
EXPLORE NAVIGATABLESTRING OBJECTS
PARSE DATA
WEB SCRAPE IN PRACTICE

CONCLUSION

----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------
INTRODUCTION
WELCOME
WHAT YOU SHOULD KNOW
GETTING STARTED WITH JUPYTER

----------------------------------------------------------------------------------------------------------------------
1. DATA MUNGING BASICS
----------------------------------------------------------------------------------------------------------------------
FILTER AND SELECT DATA
TREAT MISSING VALUES
REMOVE DUPLICATES
CONCATENATE AND TRANSFORM DATA
GROUP AND AGGREGATE DATA

----------------------------------------------------------------------------------------------------------------------
QUIZ
1. The series object series_obj has 5 items with the index labels `row 1` through `row 5`, and the integer indexes 0 through 4 respectively. The command series_obj[`row 3`] is equivalent to series_obj[2] .

2. In the dropna() DataFrame method, what will the argument axis=1 cause?
	drop columns that contain missing values, instead of rows

3. You call the method duplicated() on an object and get a True value for row 8. This means that this row is unique and has no duplicates.
	FALSE

4. What should you consider when appending a DataFrame to itself?
	Indexes will be duplicated and therefore inconsistent.

----------------------------------------------------------------------------------------------------------------------	
----------------------------------------------------------------------------------------------------------------------
2. DATA VISUALIZATION BASICS
----------------------------------------------------------------------------------------------------------------------
CREATE STANDARD LINE, BAR, AND PIE PLOTS
DEFINE PLOT ELEMENTS
FORMAT PLOTS
CREATE LABELS AND ANNOTATIONS
CREATE VISUALIZATIONS FROM TIME SERIES DATA
CONSTRUCT HISTOGRAMS, BOX PLOTS, AND SCATTER PLOTS


QUIZ
1. To plot a line chart, use the plot function.
2. In your plot, you use add_axes([.5, .5, 1, 1]). Where will the axes be placed inside the figure area?
	top-right corner
3. Which argument controls the marker edge width?
	mew
4. The xy parameter is part of the annotate() function.
	TRUE
5. The Seaborn equivalent of plot(kind=`scatter`) is regplot() .


----------------------------------------------------------------------------------------------------------------------
3. BASIC MATH AND STATISTICS
----------------------------------------------------------------------------------------------------------------------
USE NUMPY ARITHMETIC
Creating arrays
np.random.seed(25)
np.random.randn(6) #array([ 0.23,  1.03, -0.84, -0.59, -0.96, -0.22])
c = 36*np.random.randn(6) #array([ 38.04, -15.11,  82.61, -93.4 , 101.62,  24.51])
d = np.arange(1,35) #array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34])
arthimetic on arrays
a * 10
c + a
c - a
c * a
c / a
Multiplying matrices
aa*bb
np.dot(aa,bb)


GENERATE SUMMARY STATISTICS-(DESCRIPTIVE)
decribe a variable`s numeric values
cars.sum()
cars.sum(axis=1)
cars.median()
cars.mean()
cars.max()
mpg.idxmax()

describe variable distribution
cars.std()
cars.var()
cars.gear.value_counts()
cars.describe()


SUMMARIZE CATEGORICAL DATA
cars.carb.value_counts()
cars.describe()

# describe with group by column,the  mean mediean mode, 5,50,75% count min max for each other column.
gears_group = cars_cat.groupby(`gear`)
gears_group.describe()

Convert int into category
pd.Series(cars.gear, dtype="category")

categorical data with crosstabs
pd.crosstab(cars[`am`], cars[`gear`])


PARAMETRIC METHODS
-Exploring correlation between variables
	- parametric methods in pandas and scipy
		-The Pearson Correlation
			sb.pairplot(cars)
			pd.plotting.scatter_matrix(mouse); 

			import scipy
			from scipy.stats.stats import pearsonr
			pearsonr_coefficient, p_value = pearsonr(mpg, hp)
			print(`mpg, hp PearsonR Correlation Coefficient %0.3f` % (pearsonr_coefficient))
			print(f`mpg, hp p_value {p_value:.7f}`)
			#calculate the Pearson correlation coefficient
			corr = X.corr()
			#visualize the Pearson correlation coefficient
			sb.heatmap(corr,xticklabels=corr.columns.values, yticklabels=corr.columns.values)

		Pearson Correlation Assumptions
		- Data is normally distributed
		- You have continous, numeric values
		- Your variables are linearly related
NON-PARAMETRIC METHODS
	- Used for Categorical, Non Linearly related, Non Normally distributed variables
		1. Spearman`s rank  correlation - Ordinale Data types- Numerical variables that can be categorized -->  1 0 -1
			Spearman`s rank Correlation Assumptions
				a. Ordinal Variables ( Numeric but may be ranked like a categorical variable)
				b. Related Non Linearly
				c. non-normally distributed
			
			from scipy.stats import spearmanr
			spearmanr_coefficient, p_value = spearmanr(cyl, vs)
			print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
			>>> Spearman Rank Correlation Coefficient -0.814  					STRONG Negative Correlation

			spearmanr_coefficient, p_value = spearmanr(cyl, am)
			print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
			Spearman Rank Correlation Coefficient -0.522    							Negative Correlation

			spearmanr_coefficient, p_value = spearmanr(cyl, gear)
			print(`Spearman Rank Correlation Coefficient %0.3f` % (spearmanr_coefficient))
			Spearman Rank Correlation Coefficient -0.564   								Negative Correlation



		2. Chi-Square tables - Test for independence between variables -  Null hypothesis= Variables are independent.
			p < 0.5 - Reject Null hypothesis and conclude that the variables are correlated
			p > 0.5 - Accept Null hypothesis and conclude that the variables are "INDEPENDENT"
			Chi-Square Correlation Assumptions
				a. Make sure if variables are categorical or Numeric
				b. If numerical, make sure you have binned them( variable has numeric values 0 - 100, bin them in bins of 10 like 0-10, 11-20, ...91-100 )

			table = pd.crosstab(cyl, am)
			from scipy.stats import chi2_contingency

			chi2, p, dof, expected = chi2_contingency(table.values)
			print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
			>>>Chi-square Statistic 8.741 p_value 0.013

			table = pd.crosstab(cars[`cyl`], cars[`vs`])
			chi2, p, dof, expected = chi2_contingency(table.values)
			print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
			>>>Chi-square Statistic 21.340 p_value 0.000


			table = pd.crosstab(cars[`cyl`], cars[`gear`])
			chi2, p, dof, expected = chi2_contingency(table.values)
			print(`Chi-square Statistic %0.3f p_value %0.3f` % (chi2, p))
			>>>Chi-square Statistic 18.036 p_value 0.001

			"p < 0.05 so Reject the Null Hypothesis and conclude that variables are correlated and not independent"

TRANSFORM DATASET DISTRIBUTIONS
1. Normalization - 			Value of Observation
				  -------------------------------------
					Sum of All Observation in variable

2. Standardization - Rescaling Data so it has a zero mean and unit Variance

- Normalizing and transforming features with MinMaxScalar() and fit_transform()
import sklearn
from sklearn import preprocessing
mpg_matrix = mpg.values.reshape(-1,1)
		>>>
		array([21. , 21. , 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8, 19.2, 17.8,
		       16.4, 17.3, 15.2, 10.4, 10.4, 14.7, 32.4, 30.4, 33.9, 21.5, 15.5,
		       15.2, 13.3, 19.2, 27.3, 26. , 30.4, 15.8, 19.7, 15. , 21.4])
		to 
		array([[21. ],[21. ],[22.8],[21.4],[18.7],[18.1],[14.3],[24.4],[22.8],[19.2],[17.8],[16.4],[17.3],[15.2],[10.4],[10.4],[14.7],[32.4],[30.4],[33.9],[21.5],[15.5],[15.2],[13.3],[19.2],[27.3],[26. ],[30.4],[15.8],[19.7],[15. ],[21.4]])

scaled = preprocessing.MinMaxScaler()
scaled_mpg = scaled.fit_transform(mpg_matrix)
		>>>
		array([[0.45106383],[0.45106383],[0.52765957],[0.46808511],[0.35319149],[0.32765957],[0.16595745],[0.59574468],[0.52765957],[0.37446809],[0.31489362],[0.25531915],[0.29361702],[0.20425532],[0.        ],[0.        ],[0.18297872],[0.93617021],[0.85106383],[1.        ],[0.47234043],[0.21702128],[0.20425532],[0.12340426],[0.37446809],[0.71914894],[0.66382979],[0.85106383],[0.22978723],[0.39574468],[0.19574468],[0.46808511]])
plt.plot(scaled_mpg)

# Set Y values Range instead of default 0 to 1
mpg_matrix = mpg.values.reshape(-1,1)
scaled = preprocessing.MinMaxScaler(feature_range=(0,10))
scaled_mpg = scaled.fit_transform(mpg_matrix)
plt.plot(scaled_mpg)


-  Using scale() to scale your features
from sklearn.preprocessing import scale
standardized_mpg = scale(mpg, axis=0, with_mean=False, with_std=False) # Y axis is same 
plt.plot(standardized_mpg) 

standardized_mpg = scale(mpg) # Y axis is changed into - and + values
plt.plot(standardized_mpg)




QUIZ
1. You have two 2x2 matrices; one has 1`s for all values, and the other has 2`s for all values. Their NumPy dot multiplication will produce a 2x2 matrix with 4`s for all values .
3. Which Pearson correlation requirement will not be satisfied when analyzing the correlation between people`s height and age?
	The relationship is linear.
	"Non-linear relationships have an apparent pattern, just not linear. For example, as age increases height increases up to a point then levels off after reaching a maximum height."
4. In which way is the Spearman`s Rank analysis similar to Pearson`s?
	It produces R-values between -1 and 1

----------------------------------------------------------------------------------------------------------------------
4. DIMENSIONALITY REDUCTION
----------------------------------------------------------------------------------------------------------------------
INTRODUCTION TO MACHINE LEARNING
"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch04/ML_InThisCourse.PNG"

EXPLANATORY FACTOR ANALYSIS
Factor Analysis Assumptions 
1. you`re features are metric,
2. they are either continuous or ordinal,
3. you have a correlation coefficient R greater than 0.3,
4. you have more than 100 observations, and more than five observations per feature. 
5. It also assumes that you`re sample is homogenous. 

--------------------------------CODE- START-----------------------------
iris = datasets.load_iris()
X = iris.data
variable_names = iris.feature_names

factor = FactorAnalysis().fit(X)

pd.DataFrame(factor.components_, columns=variable_names)
	sepal length	sepal width 	petal length	petal width
0	0.707227		-0.153147		1.653151		0.701569
1	0.114676		0.159763		-0.045604		-0.014052
2	-0.000000		0.000000		0.000000		0.000000
3	-0.000000		0.000000		0.000000		-0.000000
--------------------------------CODE - END------------------------------
	
PRINCIPAL COMPONENT ANALYSIS (PCA)

Singular Value Decomposition-(SVD) is a linear algebra method that you use to decompose a matrix into three resultant matrices. You do this in order to reduce information redundancy and noise. SVD is most commonly used for principle component analysis.

This is our original data set, it`s called A, and we decompose it into three resultant matrices, U, S and V. 
	U is the "left  orthogonal matrix", and it holds all of the important non-information-redundant information about the observations in the original data set. 
	V is the "right orthogonal matrix", and it holds all of the important non-redundant information on features in the original data set. 
	S, this is the "diagonal matrix", and it contains all of the information about the decomposition processes that were performed during the compression.

PCA USE-CASES :
	You can use PCA for 
		fraud detection, 
		spam detection, 
		image recognition, 
		speech recognition, or 
		outlier removal, if you are using PCA for data pre-processing. 

----------------------------CODE- START------------------------------------------
iris_pca = PCA().fit_transform(X)

pca.explained_variance_ratio_ #EXPLAINED VARIANCE RATION
array([0.92461621, 0.05301557, 0.01718514, 0.00518309])
"Look at the explained variance ratio. We see that the first component explains 92.4% of the data set`s variation. That means it holds 92.4% of the data`s information in one principal component. Pretty cool, right? And by taking the first two components, we only elude 2.3% of the data set`s information. That`s the junk we want to get rid of anyway, so let`s do that. We will take only the first two components and feel satisfied knowing that they contain 97.7% of the iris data set`s original information. "

pca.explained_variance_ratio_.sum() #CUMMULATIVE VARIANCE RATION
1.0

comps = pd.DataFrame(pca.components_, columns=variable_names)

	sepal length (cm)	sepal width (cm)	petal length (cm)	petal width (cm)
0	0.361590				-0.082269			0.856572			0.358844
1	0.656540				0.729712			-0.175767			-0.074706
2	-0.580997				0.596418			0.072524			0.549061
3	0.317255				-0.324094			-0.479719			0.751121
--------------------------CODE- END--------------------------------------------------


PCA and FACTOR Analysis Usage in MACHINE LEARNING
"YOU MAY BE WONDERING HOW YOU CAN USE FACTORS AND COMPONENTS. WELL, BOTH FACTORS AND COMPONENTS REPRESENT WHAT IS LEFT OF THE DATA SET AFTER INFORMATION REDUNDANCY AND NOISE HAVE BEEN STRIPPED OUT. YOU USE THEM AS INPUT VARIABLES FOR MACHINE LEARNING ALGORITHMS TO GENERATE PREDICTIONS FROM THESE COMPRESSED REPRESENTATIONS OF YOUR DATA."

 The results from this correlation heat map show that, 
 1. principal component one is strongly positively correlated with petal length and moderately positively correlated with sepal length and petal width. Component one is slightly negatively correlated with sepal width,  
 2. principal component number two is strongly negatively correlated with sepal length and sepal width, and slightly negatively correlated with petal length and petal width. You may be wondering how you can use these components once you`ve isolated. Well, you can use them as input variables for machine learning algorithms. So in the case of the iris data set, you could use the two components we`ve generated as input for a logistic regression model to predict species labels for new incoming data points. 

QUIZ
1. How much of your data should go to the test set?
	one-third
2. The goal of PCA is to keep only the principal components that matter. 70%
----------------------------------------------------------------------------------------------------------------------
----------------------------------------------------------------------------------------------------------------------
5. OUTLIER ANALYSIS
----------------------------------------------------------------------------------------------------------------------
EXTREME VALUE ANALYSIS USING UNIVARIATE METHODS
Types of Outliers
1. Point Outliers - Univariate Outliers
2. Contextual Outliers - Location,Temperature
3. Collective Outliers - These are a collection of observations that are anomalous but appear close to one another because they all have similar anomalous values. - DBSCAN
			(anomalous - deviating from what is standard, normal, or expected.)

Use of Outlier detection( Basically used to uncover anomalies in data)
	1. equipment failure notification, 
	2. fraud detection, and 
	3. cybersecurity event notification
UNIVARIATE METHODS: 
	Tukey Boxlplots - You can detect unusually high or low data points in a variable by applying the Tukey methods for outlier detection
		Boxplot whiskers - IQR Inter Quantile Range
		"https://github.com/Gurubux/LinkedIn-Learn/blob/master/1-MasterPythonForDataScience/1-3-PythonForDataScienceEssentialTraining/Ex_Files_Python_Data_Science_EssT/Exercise%20Files/Ch05/05_01/Univariate_Boxplot_Tukey.PNG"
df.boxplot(return_type='dict')
plt.plot()

def tukey_label(data):
    IQR = np.quantile(data, 0.75) - np.quantile(data, 0.25)
    Q1 = np.quantile(data, 0.25)
    Q3 = np.quantile(data, 0.75)
    #print(f"{IQR:.2f},{Q1:.2f},{Q3:.2f}")
    outlier_1 = Q1 - (IQR * 1.5)
    outlier_2 = Q3 + (IQR * 1.5)
    return (outlier_1,outlier_2)

Sepal_Width = X[:,1]
iris_outliers = (Sepal_Width > 4)
df[iris_outliers]

Sepal_Width = X[:,1]
iris_outliers = (Sepal_Width < 2.05)
df[iris_outliers]



MULTIVARIATE ANALYSIS FOR OUTLIER DETECTION
A LINEAR PROJECTION METHOD FOR MULTIVARIATE DATA

----------------------------------------------------------------------------------------------------------------------
6. CLUSTER ANALYSIS
----------------------------------------------------------------------------------------------------------------------
K-MEANS METHOD
HIERARCHICAL METHODS
INSTANCE-BASED LEARNING WITH K-NEAREST NEIGHBOR

----------------------------------------------------------------------------------------------------------------------
7. NETWORK ANALYSIS WITH NETWORKX
----------------------------------------------------------------------------------------------------------------------
INTRO TO NETWORK ANALYSIS
WORK WITH GRAPH OBJECTS
SIMULATE A SOCIAL NETWORK
GENERATE STATS ON NODES AND INSPECT GRAPHS

----------------------------------------------------------------------------------------------------------------------
8. BASIC ALGORITHMIC LEARNING
----------------------------------------------------------------------------------------------------------------------
LINEAR REGRESSION MODEL
LOGISTIC REGRESSION MODEL
NAÏVE BAYES CLASSIFIERS

----------------------------------------------------------------------------------------------------------------------
9. WEB-BASED DATA VISUALIZATIONS WITH PLOTLY
----------------------------------------------------------------------------------------------------------------------
CREATE BASIC CHARTS
CREATE STATISTICAL CHARTS
CREATE PLOTLY CHOROPLETH MAPS
CREATE PLOTLY POINT MAPS

----------------------------------------------------------------------------------------------------------------------
10. WEB SCRAPING WITH BEAUTIFUL SOUP
----------------------------------------------------------------------------------------------------------------------
INTRODUCTION TO BEAUTIFUL SOUP
EXPLORE NAVIGATABLESTRING OBJECTS
PARSE DATA
WEB SCRAPE IN PRACTICE

----------------------------------------------------------------------------------------------------------------------
CONCLUSION
----------------------------------------------------------------------------------------------------------------------