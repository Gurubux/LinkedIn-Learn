Building a Recommendation System with Python Machine Learning & AI
1. SIMPLE APPROACHES TO RECOMMENDATION SYSTEMS
INTRODUCING CORE CONCEPTS OF RECOMMENDATION SYSTEMS
POPULARITY-BASED RECOMMENDERS
EVALUATING SIMILARITY BASED ON CORRELATION
CHAPTER QUIZ

2. MACHINE LEARNING RECOMMENDATION SYSTEMS
CLASSIFICATION-BASED COLLABORATIVE FILTERING
MODEL-BASED COLLABORATIVE FILTERING SYSTEMS
CONTENT-BASED RECOMMENDER SYSTEMS
EVALUATING RECOMMENDATION SYSTEMS
CHAPTER QUIZ

Things to learn:
	1. the popularity based recommender, 
	2. both types of collaborative filtering systems,
	3. content based recommenders 
	4. plus some other tools and techniques. 

************************************************************************************************************
1. SIMPLE APPROACHES TO RECOMMENDATION SYSTEMS
************************************************************************************************************
INTRODUCING CORE CONCEPTS OF RECOMMENDATION SYSTEMS
A. Collaborative filtering systems
 	Collaborative filtering systems recommend items based on how well users prefer those items over others. 
 	It`s based on crowdsourced user preference data. 
 	Two approaches of collaborative filtering, 
 		1. user based
 			They generate recommendations based on similarity between items with respect to user ratings of those items.
 			In chapter two, you`ll learn how to build a user-based collaborative filtering model by using "logistic regression" as a classifier.
 			For our example where an ecommerce site wants to recommend an item for purchase, the recommendation would come in terms of customers who are similar to you liked X, Y, or Z product so you might like this also.
 				Life-insurance Policy

 		2. item based.
 			They generate recommendations based on similarity between items with respect to user ratings of those items.
 			In this course, you`re going to see how to use "Pearson correlation" as the driver for an item-based recommender.
 			Eg: User A and B give 5 ratings to Mobile and Mobile Cover. User C gives 5 rating to Mobile, so based on A and B`s preference, we can recommend Mobile Cover to C

B. Content-based recommenders 
	These recommenders recommend items based on their features and how similar those are to features of other items in a dataset. 
	Eg : Pandora Radio uses content-based filtering to make its music recommendations.
	Later on, you`re going to see how to make content-based recommendations using the "nearest neighbor" algorithm. 

C. Popularity-based recommendation systems.
------------------------------------------------------------------------------------------------------------------------------
POPULARITY-BASED RECOMMENDERS
Popularity-based recommenders offer a very primitive form of collaborative filtering, where items are recommended to users based on how popular those items are among other users.
The assumption is that the places that have the most number of ratings or reviews are the most popular. 
So, based on popularity, Place 1 would be recommended over Place 2.
TYPES OF DATA THAT POPULARITY RECOMMENDERS USE. 
	- Rely on purchase history. 
	- Often used by news sites, like Bloomberg or New York Times. 
	- Don`t take user data into account, thus cannot make personalized recommendations

Recommedation based on - "HOW POPULAR THE ITEM IS AMONG THE USERS"


------------------------------------------------------------------------------------------------------------------------------
EVALUATING SIMILARITY BASED ON CORRELATION

Correlation based recommendations System
	- Use Pearson`s R correlation to recommend an item that is most similar to the item a" user has already chosen."
	- Item-based similarity: How correlated are two items based on user ratings

In our example :
	Items - Places to eat 
	Users - Restaurant goers
\GROUPING AND RANKING DATA

	placeID	name
121	135085	Tortas Locas Hipocampo

	placeID	Rcuisine
44	135085	Fast_Food

"Tortas Locas Hipocampo has highest number of ratings and the Cusine it serves is Fast-Food"

\PREPARING DATA FOR ANALYSIS

places_crosstab = pd.pivot_table(data=frame, values='rating', index='userID', columns='placeID')
places_crosstab.head()
places_crosstab
>>>
placeID	132560	132561	132564	132572	132583	132584	132594	132608	132609	132613	...	135080	135081	135082	135085	135086	135088	135104	135106	135108	135109
userID																					
U1001	NaN		NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN											...	NaN		NaN		NaN		0.0		NaN		NaN		NaN		NaN		NaN		NaN
U1002	NaN		NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN											...	NaN		NaN		NaN		1.0		NaN		NaN		NaN		1.0		NaN		NaN
U1003	NaN		NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN											...	2.0		NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN
U1004	NaN		NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN											...	NaN		NaN		NaN		NaN		NaN		NaN		NaN		2.0		NaN		NaN
U1005	NaN		NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN											...	NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN
U1006	NaN		NaN	NaN	1.0	NaN	NaN	NaN	NaN	NaN	NaN											...	NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN
U1007	NaN		NaN	NaN	1.0	NaN	NaN	NaN	NaN	NaN	NaN											...	NaN		NaN		NaN		1.0		0.0		NaN		NaN		NaN		1.0		NaN
U1008	NaN		NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN											...	NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN		1.0		NaN
U1009	NaN		NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN											...	NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN
U1010	NaN		NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN	NaN											...	NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN		NaN

Tortas_ratings = places_crosstab[135085]
Tortas_ratings
>>>
userID
U1001    0.0
U1002    1.0
U1003    NaN
U1004    NaN
U1005    NaN
U1006    NaN
U1007    1.0
U1008    NaN
U1009    NaN
U1010    NaN
Name: 135085, dtype: float64 - (130,)


\EVALUATING SIMILARITY BASED ON CORRELATION

similar_to_Tortas = places_crosstab.corrwith(Tortas_ratings)
>>>
placeID
132560         NaN
132561         NaN
132564         NaN
132572   -0.428571
132583         NaN
132584         NaN
132594         NaN
132608         NaN
132609         NaN
132613         NaN
dtype: float64

corr_Tortas = pd.DataFrame(similar_to_Tortas, columns=['PearsonR'])
corr_Tortas.dropna(inplace=True)
corr_Tortas.sort_values('PearsonR', ascending=False).head(10)
>>>
		PearsonR
placeID	
135076	1.000000
132922	1.000000
135085	1.000000
132937	1.000000
132925	1.000000
135066	1.000000
135053	1.000000
132754	0.930261
135045	0.912871
135062	0.898933
ratings(45, 1)

rating['rating_count']
>>>
placeID
132560     4
132561     4
132564     4
132572    15
132583     4
Name: rating_count, dtype: int64 - (130,)


Tortas_corr_summary = corr_Tortas.join(rating['rating_count'])
(45, 2)

Tortas_corr_summary[Tortas_corr_summary['rating_count']>=10].sort_values('PearsonR', ascending=False).head(10)
>>>
		PearsonR	rating_count
placeID		
135076	1.000000	13
135085	1.000000	36
135066	1.000000	12
132754	0.930261	13
135045	0.912871	13
135062	0.898933	21
135028	0.892218	15
135042	0.881409	20
135046	0.867722	11
132872	0.840168	12


places_corr_Tortas = pd.DataFrame([135085, 132754, 135045, 135062, 135028, 135042, 135046], index = np.arange(7), columns=['placeID'])
summary = pd.merge(places_corr_Tortas, cuisine,on='placeID')
>>>
	placeID	 Rcuisine
0	135085 "Fast_Food"  # This is tortas
1	132754	 Mexican
2	135028	 Mexican
3	135042	 Chinese
4	135046 "Fast_Food"  # FInd this Place

places[places['placeID']==135046]
	placeID	name
42	135046	Restaurante El Reyecito
------------------------------------------------------------------------------------------------------------------------------
CHAPTER QUIZ
1. The core job of a recommender is to relate users to items.
2. Why can`t popularity-based recommenders produce personalized results?
	because they`re based only on popularity counts of items, and not on user attribute data
3. How are items recommended with item-based correlation recommenders?
	Similar items are recommended based on their degree of correlation with respect to user ratings


************************************************************************************************************
2. MACHINE LEARNING RECOMMENDATION SYSTEMS
************************************************************************************************************
CLASSIFICATION-BASED COLLABORATIVE FILTERING
Naive Bayes, Logistic Regression
They are able to make personalized recommendations

TYPES OF DATA THAT CLASSIFICATION-BASED COLLABORATIVE FILTERING RECOMMENDERS USE. 
	- Transaction history
	- User attribute data
	- Contextual Data - Hour, Season or User browsing history

CODE
User-based Collaborative Filtering
Logistic Regression as a Classifier
"Bank Marketing Data Set"
The classification goal is to predict if the client will subscribe (yes/no) a term deposit (variable y).


X = bank_full.iloc[:,18:37].values
>>>
Index(['housing_loan                ', 'credit_in_default', 'personal_loans',
       'prev_failed_to_subscribe    ', 'prev_subscribed             ',
       'job_management              ', 'job_tech                    ',
       'job_entrepreneur            ', 'job_bluecollar              ',
       'job_unknown                 ', 'job_retired                 ',
       'job_services                ', 'job_self_employed           ',
       'job_unemployed              ', 'job_maid                    ',
       'job_student                 ', 'married                     ',
       'single                      ', 'divorced                    '],
      dtype='object')
*Note* these are dummy variables For 
	1. Previously_Subscribed y/n
	2. Job_type 
	3. Maritial_status m/s/d
	4. Housing Loan y/n
	5. Personal Loan  y/n
	6. Credit in default  y/n


y = bank_full.iloc[:,17].values # y_binary term deposit y/n = 1/0

LogReg = LogisticRegression()
LogReg.fit(X, y)


new_user = [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1]
'housing_loan', 'credit_in_default', 'personal_loans','prev_failed_to_subscribe', 'prev_subscribed','job_management'✔, 'job_tech','job_entrepreneur', 'job_bluecollar',
 'job_unknown ', 'job_retired ','job_services', 'job_self_employed','job_unemployed', 'job_maid','job_student ', 'married','single'✔, 'divorced'✔

y_pred = LogReg.predict(new_user)
y_pred
>>> array([0], dtype=int64)

------------------------------------------------------------------------------------------------------------------------------
MODEL-BASED COLLABORATIVE FILTERING SYSTEMS
With these systems you build a model from user ratings, and then make recommendations based on that model. 
SVD - Singular Value Decomposition. 
Utility matrix is also known as user item matrix.
	Utility matrix  are sparse

SVD 
	- Is a linear algebra method that you can use to decompose a utility matrix into three compressed matrices. 
	- It`s useful For building a model-based recommender because you can use these "compressed matrices" to make recommendations without having to refer back to the complete And entire dataset. 
	- With SVD, you uncover latent variables. These are inferred variables that are present within And affect the behavior of a dataset. Although these variables are present And influential within a dataset, They`re not directly observable.

Anatomy of SVD = "https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/2-AdvanceYourSkills_as_aPythonDataExpert/2-3-BuildingRecommendationSystemWithPythonMachineLearningAI/SVD_Anatomy.PNG"
Anatomy of Truncated SVD = "https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/2-AdvanceYourSkills_as_aPythonDataExpert/2-3-BuildingRecommendationSystemWithPythonMachineLearningAI/Truncated_SVD_Anatomy.PNG"


SVD Matrix Factorization
Dataset - MovieLens 100K movie ratings
"PREPARING THE DATA"
"BUILDING A UTILITY MATRIX"
"TRANSPOSING THE MATRIX"
"GENERATING A CORRELATION MATRIX"
"ISOLATING STAR WARS FROM THE CORRELATION MATRIX"
"RECOMMENDING A HIGHLY CORRELATED MOVIE"



from sklearn.decomposition import TruncatedSVD

"PREPARING THE DATA"
frame
>>>
	user_id	item_id	rating	timestamp
0	196	242	3	881250949
1	186	302	3	891717742
2	22	377	1	878887116
3	244	51	2	880606923
4	166	346	1	886397596

movies_name
>>>
	item_id	movie title
0	1		Toy Story (1995)
1	2		GoldenEye (1995)
2	3		Four Rooms (1995)
3	4		Get Shorty (1995)
4	5		Copycat (1995)

combined_movies_data = pd.merge(frame, movie_names, on='item_id')
>>>
	user_id	item_id	rating	timestamp	movie title
0	196	242	3	881250949	Kolya (1996)
1	63	242	3	875747190	Kolya (1996)
2	226	242	5	883888671	Kolya (1996)
3	154	242	3	879138235	Kolya (1996)
4	306	242	5	876503793	Kolya (1996)

Groupby and get the movie with highest amount of rating
combined_movies_data.groupby('item_id')['rating'].count().sort_values(ascending=False).head()
item_id
"50"     583
258    509
100    508
181    507
294    485
Name: rating, dtype: int64

combined_movies_data[combined_movies_data['item_id']==50]['movie title'].unique()
>>> array(['Star Wars (1977)'], dtype=object)




"BUILDING A UTILITY MATRIX"
rating_crosstab = combined_movies_data.pivot_table(values='rating', index='user_id', columns='movie title', fill_value=0)
rating_crosstab.head()
movie title	`Til There Was You (1997)	1-900 (1994)	101 Dalmatians (1996)	12 Angry Men (1957)	187 (1997)	2 Days in the Valley (1996)	20,000 Leagues Under the Sea 			(1954)	2001: A Space Odyssey (1968)	3 Ninjas: High Noon At Mega Mountain (1998)	39 Steps, The (1935)	...	Yankee Zulu (1994)	Year of the Horse (1997)			You So Crazy (1994)	Young Frankenstein (1974)	Young Guns (1988)	Young Guns II (1990)	Young Poisoner`s Handbook, The (1995)	Zeus and Roxanne (1997)				unknown	Á köldum klaka (Cold Fever) (1994)
user_id																					
1			0	0	2	5	0	0	3	4	0	0	...	0	0	0	5	3	0	0	0	4	0
2			0	0	0	0	0	0	0	0	1	0	...	0	0	0	0	0	0	0	0	0	0
3			0	0	0	0	2	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
4			0	0	0	0	0	0	0	0	0	0	...	0	0	0	0	0	0	0	0	0	0
5			0	0	2	0	0	0	0	4	0	0	...	0	0	0	4	0	0	0	0	4	0
(943, 1664)
X = rating_crosstab.T
(1664, 943)

"TRANSPOSING THE MATRIX"
SVD = TruncatedSVD(n_components=12, random_state=17)

resultant_matrix = SVD.fit_transform(X)
(1664, 12)

✔ ----> We want to use the similarities between users, to decide which movies to recommend, so we can use truncated SVD to compress all of the user ratings down to just 12 latent variables. These variables are going to capture most of the information that was stored in the 943 user columns previously. They represent a generalized view of users` tastes and preferences.



"GENERATING A CORRELATION MATRIX"
corr_mat = np.corrcoef(resultant_matrix)
(1664, 1664)

"ISOLATING STAR WARS FROM THE CORRELATION MATRIX"
movie_names = rating_crosstab.columns
star_wars_index = list(movie_names).index('Star Wars (1977)')
>>>1398

corr_star_wars = corr_mat[1398] # array([0.36854685, 0.42380505, 0.59468319, ..., 0.33301726, 0.64177278,0.26805079])
(1664,)

"RECOMMENDING A HIGHLY CORRELATED MOVIE"

list(movie_names[(corr_star_wars<1.0) & (corr_star_wars > 0.9)])
>>>
['Die Hard (1988)',
 'Empire Strikes Back, The (1980)',
 'Fugitive, The (1993)',
 'Raiders of the Lost Ark (1981)',
 'Return of the Jedi (1983)',
 'Terminator 2: Judgment Day (1991)',
 'Terminator, The (1984)',
 'Toy Story (1995)']

list(movie_names[(corr_star_wars<1.0) & (corr_star_wars > 0.95)])
['Return of the Jedi (1983)']



------------------------------------------------------------------------------------------------------------------------------
CONTENT-BASED RECOMMENDER SYSTEMS
Nearest neighbor algorithm
	Unsupervised Classifier
	Also known as memorey-based system
	   Because it memorizes instances and then recommends an item, or a single instance, based on how quantitatively similar it is to a new incoming instance. 
	Example : MtCars
		You get a customer that comes in and tells you that he wants a car that gets 25 miles per gallon and has a 4.7 liter, 425 horsepower engine. You have a dataset that describes all of these specs on the cars in your inventory. So you could use the nearest neighbor algorithm to identify the single best matching car from among all cars in your inventory database. To do this, you`d create a single test point that represents your customer`s desired specifications. When you run the model, it will memorize all of the data points in your inventory. It will then calculate the single data point that is quantitatively most similar to your test point. The car that this data point represents would be the one "YOU`D RECOMMEND TO YOUR CUSTOMER. "

from sklearn.neighbors import NearestNeighbors

cars = pd.read_csv('mtcars.csv')
cars.columns = ['car_names', 'mpg', 'cyl', 'disp', 'hp', 'drat', 'wt', 'qsec', 'vs', 'am', 'gear', 'carb']
cars.head()

t = [15, 300, 160, 3.2]

X = cars.iloc[:,(1, 3, 4, 6)].values # mpg , disp , hp , wt

nbrs = NearestNeighbors(n_neighbors=1).fit(X)

print(nbrs.kneighbors([t]))
>>> (array([[ 10.77474942]]), array([[22]], dtype=int64))


------------------------------------------------------------------------------------------------------------------------------
EVALUATING RECOMMENDATION SYSTEMS


		  		No of items I Liked and were also recommended
PRECISION =	------------------------------------------------
					Total Items Recommended
		  = Relevance ? How relevant were your recommendations?

		  		No of items I Liked and were also recommended
RECALL 	  =	------------------------------------------------
					Total Items I Liked

		  = Completeness ? How Completely did the recommender system predict the items I liked ? 



bank_full = pd.read_csv('bank_full_w_dummy_vars.csv')

X = bank_full.iloc[:,18:37].values
y = bank_full.iloc[:,17].values

LogReg = LogisticRegression()
LogReg.fit(X, y)

y_pred = LogReg.predict(X)

print(classification_report(y, y_pred))
            precision    recall  f1-score   support

          0       0.90      0.99      0.94     39922
          1       0.67      0.17      0.27      5289

avg / total      "0.87"    "0.89"     0.86     45211



So we see we have a precision of 87 here, and what that means is that of all the offers that were made, 87% of them were made to users that liked them. This metric is an indicator of how precise the predictions were. 

When we look over at recall we see that we get an 89. And what that is really saying is of all the products that users liked, 89% of those products were offered to them. So in other words, recall is expressing how complete the predictions were that the model made or model completeness. 
------------------------------------------------------------------------------------------------------------------------------
CHAPTER QUIZ
1. Classification-based recommenders are NOT able to accept product specification data .
2. How do user-based collaborative filtering systems make recommendations?
	by considering user behavior and similarities between each of the users
3. Content-based systems recommend based on user attributes. - False
4. How can you determine a recommendation`s relevancy?
	by looking at the recommendation model`s precision

