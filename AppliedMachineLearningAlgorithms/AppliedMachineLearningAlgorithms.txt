INTRODUCTION
THE POWER OF ALGORITHMS IN MACHINE LEARNING 
WHAT YOU SHOULD KNOW 
WHAT TOOLS YOU NEED 

************************************************************************************************************
1. REVIEW OF FOUNDATIONS
************************************************************************************************************
DEFINING MODEL VS. ALGORITHM 
PROCESS OVERVIEW 
CLEAN CONTINUOUS VARIABLES 
CLEAN CATEGORICAL VARIABLES 
SPLIT INTO TRAIN, VALIDATION, AND TEST SET 

************************************************************************************************************
3. LOGISTIC REGRESSION
************************************************************************************************************
WHAT IS LOGISTIC REGRESSION? 
WHEN SHOULD YOU CONSIDER USING LOGISTIC REGRESSION? 
USED FOR :
	A binary target variable 
	Not too many outliers, 
	Not too many missing values
	Relationships that aren`t too complex. 
	Great For is an initial benchmark model on a binary classification problem with fairly well-behaved data. 
	It`s relatively flexible, 
	It`s very fast to train. 

NOT USED FOR:
	A continuous target variable. 
	Massive amount of data. (There are other algorithms that really shine when you have a ton of data. I also want to call out That`s when you have lots of features, but very few rows. Or you can have long and skinny data where you have a lot of rows, but very few features. skewed features, or really complex relationships.)
	Generally speaking it`s not going to be the best perForming algorithm on any given use case. It will usually do pretty well on any given problem, but it will rarely be the best. 
SUMMARY :
	Logistic regression is a great tool For a fast, transparent baseline model For a binary classification problem. It doesn`t do well with a lot of data or particularly messy data. It`s also unlikely to give you the best possible perFormance on any given problem.
"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/LogisticRegr_Use_NotUse.PNG"

WHAT ARE THE KEY HYPERPARAMETERS TO CONSIDER?
-  We`re going to focus on this C hyperparameter. So you can see here that has a default value of one.
-  So regularization combats this overfitting by discouraging overly complex models in some way. 
	Now, calling C a regularization parameter is actually slightly misleading as :
		
		C = 1 / λ 
			where Lambda is actually the regularization parameter. 
		λ = 0 then C = ∞ 
			that indicates low regularization and more likely to overfit. 
		λ = ∞ then C = 0
			that indicates high regularization and more likely to underfit. 
"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/LogisticRegression_Hyperparameter_regularization_C.PNG"


FIT A BASIC LOGISTIC REGRESSION MODEL 

from sklearn.linear_model import LogisticRegression
lr = LogisticRegression()
parameters = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}

cv = GridSearchCV(lr, parameters, cv=5)
cv.fit(tr_features, tr_labels.values.ravel())

print_results(cv)

def print_results(results):
    print('BEST PARAMS: {}\n'.format(results.best_params_))

    means = results.cv_results_['mean_test_score']
    stds = results.cv_results_['std_test_score']
    for mean, std, params in zip(means, stds, results.cv_results_['params']):
        print('{} (+/-{}) for {}'.format(round(mean, 3), round(std * 2, 3), params))
>>>
BEST PARAMS: {'C': 1}

0.678 (+/-0.092) for {'C': 0.001}
0.704 (+/-0.099) for {'C': 0.01}
0.796 (+/-0.13) for {'C': 0.1}
0.798 (+/-0.123) for {'C': 1}
0.794 (+/-0.118) for {'C': 10}
0.794 (+/-0.118) for {'C': 100}
0.794 (+/-0.118) for {'C': 1000}


cv.best_estimator_
>>>
LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,
          intercept_scaling=1, max_iter=100, multi_class='warn',
          n_jobs=None, penalty='l2', random_state=None, solver='warn',
          tol=0.0001, verbose=0, warm_start=False)

# Write out pickled model
joblib.dump(cv.best_estimator_, '../../../LR_model.pkl')
************************************************************************************************************
4. SUPPORT VECTOR MACHINES
************************************************************************************************************
WHAT IS SUPPORT VECTOR MACHINE?
What is SVM? 
	"SUPPORT VECTOR MACHINES" is a classifier that "finds an optimal hyperplane that maximizes the margin between two classes". So, let`s start with a very basic example with only two dimensions. So, using X1 and X2, we want a line to separate the red squares from the blue circles. So, any line shown on this plot is really a viable option. They would all perfectly separate the red squares from the blue circles. However, there has to be an optimal line, or decision boundary. We really want to capture the behavior of the data here. In other words, what line would give us the best chance to correctly classify additional examples in each of these two classes? 
	- You would probably want a line that is evenly spaced between these two classes to give a little bit of buffer for each class. So, that`s exactly what support vector machines does. The definition sounds fancy, but really, you`re just trying to find a line that`s right in the middle of your two classes, evenly spaced between the two. Mathematically, it`s defined as maximizing the margin between your decision boundary and the closest points. 

SUPPORT VECTOR - So, now you might be wondering where this "SUPPORT VECTOR" terminology comes from. 
	Support vector is the name for the perpendicular line from your decision boundary to your closest points in both classes. So, on the right, it`s the perpendicular line that goes from that green optimal hyperplane to those filled-in points, that filled-in blue circle and the filled-in red squares. of those support vectors. Now, in the example on the left, to be the ideal decision boundary. You can see how close these closest points in each class are to the decision boundary, so the algorithm wouldn`t like these as the optimal solution. "That`s what SVM is seeking to do, is maximize that distance between the decision boundary and the nearest points." 

HYPERPLANE - In our definition, we talked about finding an optimal hyperplane, but in our examples, we`ve just been using a line So, a hyperplane is just a generalized term to identify your decision boundary So, in two dimensions, that`s just a line. 

KERNEL TRICK - Up to this point, everything we`ve discussed "requires our data to be linearly separable", so we`re using a straight line to separate out examples in two-dimensional space to separate examples in a three-dimensional space. We`re not using any curved lines or curved hyperplanes. So, what happens when we have data that can`t be separated by a straight line or a hyperplane? That leads us to this really creative trick called "the kernel trick". in n-dimensional space into a higher dimension where it is linearly separable. That may kind of sound like magic, so let`s look at an example. where we`re trying to identify the presence of cancer based on gene X and gene Y. This data clearly is not linearly separable in two dimensions. We couldn`t draw a straight line here that would separate the blue from the red. We would need a circle in order to do that. However, if we were to project this data from two-dimensional space into three-dimensional space like this, all of a sudden now we can use a flat hyperplane to split our data very nicely to identify the presence of cancer. So, that`s the kernel trick, and it`s a really powerful tool for support vector machines.
"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/SVM_Kernel_trick.PNG"


WHEN SHOULD YOU CONSIDER USING SVM?
Now that we have a decent feel what SVM is on a conceptual level, let`s talk a little bit about when you should use it. I want to quickly reiterate what I mentioned when we covered logistic regression. There are way too many variables in play For this to be hard and fast rules. The more comFortable you get with each algorithm, For what algorithms work best For different types of problems. This is just meant to get you started. 
USED FOR :
	Both categorical and continuous outputs, (but I found it to not be very useful For regression.) So it really should only be a focus when you`re working on a classification problem with some kind of binary target variable. 
	When the feature-to-row ratio is very high. This is what I called short and fat data previously. It`s when you have a lot of features and relatively few rows. but SVM tends to do fairly well, and it`s actually one of its distinguishing features. 
	SVM is also quite good at untangling complex relationships. Definitely more so than logistic regression is. 
	SVM also handles data with a lot of outliers quite well. If you think about how SVM works using these support vectors it actually makes sense, because it only looks at the points closest to the line, so that outliers are kind of ignored. So if you`re working with data with a lot of outliers, and many other algorithms are struggling, you might want to consider SVM. 

NOT USED FOR :
	Feature-to-row ratio is low, you should not use it if you have a lot of rows and relatively few features. 
	SVM will take a long time to train, and the perFormance really won`t be worth it relative to other algorithms. Despite the really nice plots that we saw beFore, and this hyperplane is an extremely high-dimensional space, you can`t really get a good picture what`s going on inside of SVM. So if you care about transparency, and maybe the significance of predictors, I would probably avoid SVM. 
	If you`re looking For a quick benchmark model, SVM probably is not the right choice.
	And it takes longer than other algorithms to make predictions as well. if time or maybe compute power is a significant constraint.

SUMMARY :
	SVM should be used when you have a lot of features, but few rows, or when you have a lot of outliers, that you`re trying to untangle. But it is quite slow to train, you might want to look somewhere else.
"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/SVM_Use_NotUse.PNG"




WHAT ARE THE KEY HYPERPARAMETERS TO CONSIDER?
C
kernel
"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/SVM_Hyperparameter_regularization_C.PNG"







FIT A BASIC SVM MODEL

from sklearn.svm import SV
svc = SVC()
parameters = {
    'kernel': ['linear', 'rbf'],
    'C': [0.1, 1, 10]
}

cv = GridSearchCV(svc, parameters, cv=5)
cv.fit(tr_features, tr_labels.values.ravel())

print_results(cv)
>>>
BEST PARAMS: {'C': 0.1, 'kernel': 'linear'}

0.796 (+/-0.116) for {'C': 0.1, 'kernel': 'linear'}
0.624 (+/-0.005) for {'C': 0.1, 'kernel': 'rbf'}
0.796 (+/-0.116) for {'C': 1, 'kernel': 'linear'}
0.667 (+/-0.081) for {'C': 1, 'kernel': 'rbf'}
0.796 (+/-0.116) for {'C': 10, 'kernel': 'linear'}
0.691 (+/-0.073) for {'C': 10, 'kernel': 'rbf'}

cv.best_estimator_
>>>
SVC(C=0.1, cache_size=200, class_weight=None, coef0=0.0,
  decision_function_shape='ovr', degree=3, gamma='auto_deprecated',
  kernel='linear', max_iter=-1, probability=False, random_state=None,
  shrinking=True, tol=0.001, verbose=False)

# Write out pickled model
joblib.dump(cv.best_estimator_, '../../../SVM_model.pkl')




QUIZ:
1. The kernel trick is NOT used EVERY TIME you fit SVM to data.
2. SVM`s main competitive advantage is its performance on short-and-fat data or when there are a lot of outliers.
3. Which of the following does NOT happen with a very low value of C?
	more likely to overfit
4. Our Titanic data appears to be linearly separable.


************************************************************************************************************
4. MULTI-LAYER PERCEPTRON
************************************************************************************************************
WHAT IS A MULTI-LAYER PERCEPTRON?
So multilayer perceptron is "a classic feed-forward artificial neural network of some deep learning algorithms." Not all algorithms in deep learning use a feed-forward artificial neural network, but many do. 
Another way to look at this, is that a multilayer perceptron is "a connected series of nodes, where each node represents a function. a directed acyclic graph. Meaning that there is directionality between the nodes and no node will ever be revisited."

"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/MultiLayer_Perceptron.PNG"


WHEN SHOULD YOU CONSIDER USING A MULTI-LAYER PERCEPTRON?
USED FOR:
	classification and regression
	Categorical and continuous variable
	Complex Relationships
	High Performance
	Control over Training Process

NOT USED FOR:
	Transperency, BlackBox. Quite difficult to understand internally
	If you`re looking For a quick benchmark model, MULTI-LAYER PERCEPTRON probably is not the right choice.
	Lot of hyperparameter tuning
	Slow (but better than SVM)
	Less data

SUMMARY :
	 So to summarize, multilayer perceptrons are often the best tool For the job when you have a lot of data and you really only care about performance. But they`re not ideal if you have limited data, if you don`t have much time or compute power, or you really care about the transparency of the model in understanding its predictions.
"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/MultiLayer_Perceptron_Use_NotUse.PNG"



WHAT ARE THE KEY HYPERPARAMETERS TO CONSIDER?
"activation" 		- 
	Sigmoid - a sigmoid curve, otherwise known as a logistic curve - familiar S-shaped curve from zero to one that looks exactly like what we saw with logistic regression. 
	TanH - a hyperbolic tangent curve which is referred to as TanH, this is also an S-shaped but it`s between negative one and one.
	Relu - Rectified Linear unit - It just sets a floor at zero, so if any number is below zero, it sets it to zero and if it`s above zero, then it doesn`t do anything.
			"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/MultiLayer_Perceptron_Hyperparameter_regularization_activation.PNG"

"hidden_layer_sizes" - Determines how many hidden layers there are and how many hidden nodes in each layer
			"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/MultiLayer_Perceptron_Hyperparameter_regularization_hidden_layer_sizes.PNG"


"learning_rate" 	 - The learning rate facilitates whether the algorithm will find the optimal solution and how quickly it will find the optimal solution. A high learning rate will push the algorithm along quicker to find the optimal solution but it	s possible that it pushes too far and it never actually finds the best solution. On the other hand, a lower learning rate will take much longer for the algorithm to fit, but you can have more confidence that it will find the optimal solution.
					- "constant". - So that'll just take that initial learning rate and it'll keep it the same throughout the entire optimization process.
					- "invscaling"- in other words, inverse scaling. So it gradually decreases the learning rate at each step. So this will allow it to take large jumps at first. And then it slowly decreases as it gets closer and closer to the optimal model. 
					- "adaptive"  - And this keeps the learning rate constant as long as the training loss keeps decreasing. If the learning rate stops going down then it will decrease the learning rate so that it takes smaller steps. 

			"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/MultiLayer_Perceptron_Hyperparameter_regularization_learning_rate.PNG"

from sklearn.neural_network import MLPRegressor, MLPClassifier
print(MLPRegressor())
print(MLPClassifier())
>>>
MLPRegressor(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(100,), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
       random_state=None, shuffle=True, solver='adam', tol=0.0001,
       validation_fraction=0.1, verbose=False, warm_start=False)
MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(100,), learning_rate='constant',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
       random_state=None, shuffle=True, solver='adam', tol=0.0001,
       validation_fraction=0.1, verbose=False, warm_start=False)

FIT A BASIC MULTI-LAYER PERCEPTRON MODEL

mlp = MLPClassifier()
parameters = {
    'hidden_layer_sizes': [(10,), (50,), (100,)],
    'activation': ['relu', 'tanh', 'logistic'],
    'learning_rate': ['constant', 'invscaling', 'adaptive']
}

cv = GridSearchCV(mlp, parameters, cv=5)
cv.fit(tr_features, tr_labels.values.ravel())

print_results(cv)
>>>
BEST PARAMS: {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling'}

0.764 (+/-0.118) for {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'constant'}
0.719 (+/-0.074) for {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling'}
0.704 (+/-0.132) for {'activation': 'relu', 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive'}
0.788 (+/-0.063) for {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate': 'constant'}
0.807 (+/-0.087) for {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling'}
0.779 (+/-0.118) for {'activation': 'relu', 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive'}
0.794 (+/-0.088) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}
0.781 (+/-0.088) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate': 'invscaling'}
0.801 (+/-0.116) for {'activation': 'relu', 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}
0.687 (+/-0.083) for {'activation': 'tanh', 'hidden_layer_sizes': (10,), 'learning_rate': 'constant'}
0.715 (+/-0.1) for {'activation': 'tanh', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling'}
0.693 (+/-0.107) for {'activation': 'tanh', 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive'}
0.768 (+/-0.106) for {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'learning_rate': 'constant'}
0.766 (+/-0.12) for {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling'}
0.796 (+/-0.125) for {'activation': 'tanh', 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive'}
0.796 (+/-0.09) for {'activation': 'tanh', 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}
0.807 (+/-0.088) for {'activation': 'tanh', 'hidden_layer_sizes': (100,), 'learning_rate': 'invscaling'}
0.801 (+/-0.106) for {'activation': 'tanh', 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}
0.661 (+/-0.054) for {'activation': 'logistic', 'hidden_layer_sizes': (10,), 'learning_rate': 'constant'}
0.684 (+/-0.106) for {'activation': 'logistic', 'hidden_layer_sizes': (10,), 'learning_rate': 'invscaling'}
0.706 (+/-0.083) for {'activation': 'logistic', 'hidden_layer_sizes': (10,), 'learning_rate': 'adaptive'}
0.757 (+/-0.137) for {'activation': 'logistic', 'hidden_layer_sizes': (50,), 'learning_rate': 'constant'}
0.768 (+/-0.104) for {'activation': 'logistic', 'hidden_layer_sizes': (50,), 'learning_rate': 'invscaling'}
0.764 (+/-0.132) for {'activation': 'logistic', 'hidden_layer_sizes': (50,), 'learning_rate': 'adaptive'}
0.777 (+/-0.152) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'learning_rate': 'constant'}
0.792 (+/-0.108) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'learning_rate': 'invscaling'}
0.792 (+/-0.109) for {'activation': 'logistic', 'hidden_layer_sizes': (100,), 'learning_rate': 'adaptive'}

cv.best_estimator_
>>>
MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,
       beta_2=0.999, early_stopping=False, epsilon=1e-08,
       hidden_layer_sizes=(50,), learning_rate='invscaling',
       learning_rate_init=0.001, max_iter=200, momentum=0.9,
       n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,
       random_state=None, shuffle=True, solver='adam', tol=0.0001,
       validation_fraction=0.1, verbose=False, warm_start=False)

joblib.dump(cv.best_estimator_, '../../../MLP_model.pkl')

************************************************************************************************************
5. RANDOM FOREST
************************************************************************************************************
WHAT IS RANDOM FOREST?
Random forest just merges a collection of "independent" decision trees to get a more accurate and stable prediction.
So random forest is a type of ensemble method.
Ensemble combines several machine learning models to decrease both bias and variance. 

"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/Random_Forest.PNG"
"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/Random_Forest_Testing_Voting.PNG"



WHEN SHOULD YOU CONSIDER USING RANDOM FOREST?

USED FOR :
- Categorical or continuous variable
- Interested in significance of predictors - (One of my favorite things about random forest is that it has an attribute that lays out the importance of each predictor that you`re working with. This is really helpful to understand the relationships and the power of the features that you're using in your model. )
- If you`re looking For a quick benchmark model, RANDOM FOREST probably is  the right choice.
- Messy Data, outliers


NOT USED FOR :
- Very Complex problems
- Transperency is important
- Slow


SUMMARY :
Random forest is a tremendously flexible, relatively fast tool that plays well with different data and can give you relatively good performance. 
It doesn`t always give you the best performance, but it is a really nice Swiss Army knife to have in your tool set.




WHAT ARE THE KEY HYPERPARAMETERS TO CONSIDER?
n_estimators - It simply controls how many individual decision trees are built.
max_depth 	 - hyperparameter controls how deep each of those individual decision trees can go. 

What is the main reason you sample both examples AND features for each decision tree in a Random Forest?
	encourage independence of trees

from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
print(RandomForestClassifier())
print(RandomForestRegressor())
>>>
RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,
            oob_score=False, random_state=None, verbose=0,
            warm_start=False)
RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,
           max_features='auto', max_leaf_nodes=None,
           min_impurity_decrease=0.0, min_impurity_split=None,
           min_samples_leaf=1, min_samples_split=2,
           min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,
           oob_score=False, random_state=None, verbose=0, warm_start=False)


FIT A BASIC RANDOM FOREST MODEL
rf = RandomForestClassifier()
parameters = {
    'n_estimators': [5, 50, 250],
    'max_depth': [2, 4, 8, 16, 32, None]
}

cv = GridSearchCV(rf, parameters, cv=5)
cv.fit(tr_features, tr_labels.values.ravel())

print_results(cv)
>>>
BEST PARAMS: {'max_depth': 4, 'n_estimators': 50}

0.796 (+/-0.112) for {'max_depth': 2, 'n_estimators': 5}
0.796 (+/-0.106) for {'max_depth': 2, 'n_estimators': 50}
0.798 (+/-0.124) for {'max_depth': 2, 'n_estimators': 250}
0.824 (+/-0.106) for {'max_depth': 4, 'n_estimators': 5}
0.828 (+/-0.091) for {'max_depth': 4, 'n_estimators': 50}
0.824 (+/-0.109) for {'max_depth': 4, 'n_estimators': 250}
0.807 (+/-0.087) for {'max_depth': 8, 'n_estimators': 5}
0.826 (+/-0.073) for {'max_depth': 8, 'n_estimators': 50}
0.822 (+/-0.063) for {'max_depth': 8, 'n_estimators': 250}
0.811 (+/-0.041) for {'max_depth': 16, 'n_estimators': 5}
0.816 (+/-0.04) for {'max_depth': 16, 'n_estimators': 50}
0.813 (+/-0.023) for {'max_depth': 16, 'n_estimators': 250}
0.801 (+/-0.041) for {'max_depth': 32, 'n_estimators': 5}
0.8 (+/-0.034) for {'max_depth': 32, 'n_estimators': 50}
0.813 (+/-0.032) for {'max_depth': 32, 'n_estimators': 250}
0.801 (+/-0.051) for {'max_depth': None, 'n_estimators': 5}
0.809 (+/-0.04) for {'max_depth': None, 'n_estimators': 50}
0.811 (+/-0.03) for {'max_depth': None, 'n_estimators': 250}

joblib.dump(cv.best_estimator_, '../../../RF_model.pkl')


QUIZ
1. What is the main reason you sample both examples AND features for each decision tree in a Random Forest?
	\encourage independence of trees
2. Random Forest is a tremendously flexible model that makes for a really good initial benchmark model.
3. \Increasing n_estimators or max_depth will ALWAYS decrease the training error.
4. \If I fit two Random Forest models with the exact same hyperparameter settings on the exact same data, I should get exactly the same model and performance.-False


************************************************************************************************************
6. BOOSTING
************************************************************************************************************
WHAT IS BOOSTING?
Boosting is an ensemble method that aggregates a number of weak models to create one strong model.
	Weak Model 	 - We mean a model where its predictions are only slightly better than random guessing.So for instance, if it's a binary outcome, then we roughly have a 50% chance of getting the right answer just by guessing. A weak model would only be slightly better than 50% accuracy.
	Strong Model - A strong model is one where its predictions are very strongly correlated with the actual outcome, or the true classification.

Boosting "iteratively builds" weak models where each one "learns from the previous model`s mistakes". So in the end, as a whole, it has constantly improved by learning from its own mistakes to generate really powerful predictions.

\DIFFERENCE between RANDOM FOREST and BOOSTING - (Both are Ensemble methods)
The big difference between Random Forest and boosting is that each individual model or decision tree in Random Forest was built "independently". Each tree did not know what any of the other trees were doing. That is not the case with boosting. In boosting, each successive model "learns from the mistakes of the ones before it". So they`re not independent. 
"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/Boosting.PNG"


PREDICTIONS IS PARALLELIZED BUT TRAINING ISN`T BECAUSE OF THE LEARNING ALGORITHM THAT MAKES EACH DT NON-INDEPENDENT

"https://raw.githubusercontent.com/Gurubux/LinkedIn-Learn/master/AppliedMachineLearningAlgorithms/Boosting_Testing_WeightedVoting.PNG"

WHEN SHOULD YOU CONSIDER USING BOOSTING?

USED FOR :
- Categorical or continuous variable
- Interested in significance of predictors 
- Useful on nearly any type of problem
- Prediction Fast

NOT USED FOR :
- Very Complex problems
- Training time is slow and Computer power is highly required.
- Messy Data, outliers


SUMMARY :
Gradient boosting is one of the most flexible, powerful tools out there, and honestly, you should really consider it for any problem. With that said, you do need to consider that it will take a long time to fit and you should be careful of its tendency to overfit.


WHAT ARE THE KEY HYPERPARAMETERS TO CONSIDER BOOSTING?
Gradient Boosting tree
	- n_estimators - 
	- max_depth - The trees For boosting should be shallower than the trees For random forest. 
	- learning_rate - When we use learning rate in multi layer perceptron that hyper parameter actually controlled how the learning rate changed throughout the optimization process. Remember that the three settings we looked at were "constant, inverse scaling, and adaptive." While the actual initial learning rate was controlled by a learning_rate_init parameter. Now, learning rate For gradient boosting is a little bit different as this learning rate hyperparameter controls the actual value of learning rate and it stays constant across the whole optimization process. That`s why here you can see the default value is "0.1" , instead of something like the constant or inverse scaling. 
		\In other words, this learning rate hyperparameter is the equivalent to learning rate emit in multilayer perceptron when that learning rate parameter was set to constant
	



QUIZ
1. The trees built in Gradient Boosted Trees can be fit in parallel. - False
2. Gradient Boosted Trees` ability to learn from its own mistakes also drives it to overfit to outliers.
3. A small learning rate will ensure you "always" find the optimal model. - False
4. Random Forest does better with few, deep trees, and Gradient Boosted Trees does better with many, shallow trees.




from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor
print(GradientBoostingClassifier())
print(GradientBoostingRegressor())
>>>
GradientBoostingClassifier(criterion='friedman_mse', init=None,
              learning_rate=0.1, loss='deviance', max_depth=3,
              max_features=None, max_leaf_nodes=None,
              min_impurity_decrease=0.0, min_impurity_split=None,
              min_samples_leaf=1, min_samples_split=2,
              min_weight_fraction_leaf=0.0, n_estimators=100,
              n_iter_no_change=None, presort='auto', random_state=None,
              subsample=1.0, tol=0.0001, validation_fraction=0.1,
              verbose=0, warm_start=False)
GradientBoostingRegressor(alpha=0.9, criterion='friedman_mse', init=None,
             learning_rate=0.1, loss='ls', max_depth=3, max_features=None,
             max_leaf_nodes=None, min_impurity_decrease=0.0,
             min_impurity_split=None, min_samples_leaf=1,
             min_samples_split=2, min_weight_fraction_leaf=0.0,
             n_estimators=100, n_iter_no_change=None, presort='auto',
             random_state=None, subsample=1.0, tol=0.0001,
             validation_fraction=0.1, verbose=0, warm_start=False)
FIT A BASIC BOOSTING MODEL
gb = GradientBoostingClassifier()
parameters = {
    'n_estimators': [5, 50, 250, 500],
    'max_depth': [1, 3, 5, 7, 9],
    'learning_rate': [0.01, 0.1, 1, 10, 100]
}

cv = GridSearchCV(gb, parameters, cv=5)
cv.fit(tr_features, tr_labels.values.ravel())

print_results(cv)
>>>
BEST PARAMS: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}

0.624 (+/-0.005) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 5}
0.796 (+/-0.116) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 50}
0.796 (+/-0.116) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 250}
0.811 (+/-0.118) for {'learning_rate': 0.01, 'max_depth': 1, 'n_estimators': 500}
0.624 (+/-0.005) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 5}
0.811 (+/-0.071) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}
0.83 (+/-0.076) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 250}
0.841 (+/-0.079) for {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 500}
0.624 (+/-0.005) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 5}
0.818 (+/-0.051) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 50}
0.82 (+/-0.039) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 250}
0.83 (+/-0.044) for {'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 500}
0.624 (+/-0.005) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 5}
0.818 (+/-0.054) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 50}
0.822 (+/-0.041) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 250}
0.801 (+/-0.023) for {'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 500}
0.624 (+/-0.005) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 5}
0.801 (+/-0.055) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 50}
0.801 (+/-0.024) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 250}
0.783 (+/-0.026) for {'learning_rate': 0.01, 'max_depth': 9, 'n_estimators': 500}
0.796 (+/-0.116) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 5}
0.815 (+/-0.12) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 50}
0.818 (+/-0.112) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 250}
0.828 (+/-0.093) for {'learning_rate': 0.1, 'max_depth': 1, 'n_estimators': 500}
0.813 (+/-0.073) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 5}
0.835 (+/-0.082) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}
0.831 (+/-0.038) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 250}
0.811 (+/-0.03) for {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 500}
0.815 (+/-0.053) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 5}
0.826 (+/-0.018) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 50}
0.803 (+/-0.048) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 250}
0.807 (+/-0.053) for {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 500}
0.822 (+/-0.056) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 5}
0.8 (+/-0.015) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 50}
0.794 (+/-0.044) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 250}
0.801 (+/-0.066) for {'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 500}
0.8 (+/-0.042) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 5}
0.788 (+/-0.041) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 50}
0.79 (+/-0.024) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 250}
0.79 (+/-0.049) for {'learning_rate': 0.1, 'max_depth': 9, 'n_estimators': 500}
0.818 (+/-0.1) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 5}
0.83 (+/-0.078) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 50}
0.828 (+/-0.069) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 250}
0.818 (+/-0.082) for {'learning_rate': 1, 'max_depth': 1, 'n_estimators': 500}
0.82 (+/-0.063) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 5}
0.794 (+/-0.038) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 50}
0.796 (+/-0.039) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 250}
0.801 (+/-0.048) for {'learning_rate': 1, 'max_depth': 3, 'n_estimators': 500}
0.805 (+/-0.042) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 5}
0.811 (+/-0.078) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 50}
0.809 (+/-0.074) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 250}
0.803 (+/-0.081) for {'learning_rate': 1, 'max_depth': 5, 'n_estimators': 500}
0.783 (+/-0.013) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 5}
0.787 (+/-0.051) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 50}
0.796 (+/-0.032) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 250}
0.79 (+/-0.052) for {'learning_rate': 1, 'max_depth': 7, 'n_estimators': 500}
0.785 (+/-0.034) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 5}
0.77 (+/-0.033) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 50}
0.8 (+/-0.053) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 250}
0.801 (+/-0.043) for {'learning_rate': 1, 'max_depth': 9, 'n_estimators': 500}
0.204 (+/-0.116) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 5}
0.204 (+/-0.116) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 50}
0.204 (+/-0.116) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 250}
0.204 (+/-0.116) for {'learning_rate': 10, 'max_depth': 1, 'n_estimators': 500}
0.311 (+/-0.192) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 5}
0.311 (+/-0.192) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 50}
0.311 (+/-0.192) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 250}
0.311 (+/-0.192) for {'learning_rate': 10, 'max_depth': 3, 'n_estimators': 500}
0.552 (+/-0.363) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 5}
0.444 (+/-0.316) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 50}
0.397 (+/-0.204) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 250}
0.397 (+/-0.197) for {'learning_rate': 10, 'max_depth': 5, 'n_estimators': 500}
0.599 (+/-0.171) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 5}
0.588 (+/-0.188) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 50}
0.616 (+/-0.133) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 250}
0.618 (+/-0.159) for {'learning_rate': 10, 'max_depth': 7, 'n_estimators': 500}
0.7 (+/-0.124) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 5}
0.717 (+/-0.128) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 50}
0.7 (+/-0.124) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 250}
0.704 (+/-0.11) for {'learning_rate': 10, 'max_depth': 9, 'n_estimators': 500}
0.376 (+/-0.005) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 5}
0.376 (+/-0.005) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 50}
0.376 (+/-0.005) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 250}
0.376 (+/-0.005) for {'learning_rate': 100, 'max_depth': 1, 'n_estimators': 500}
0.29 (+/-0.104) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 5}
0.29 (+/-0.104) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 50}
0.29 (+/-0.104) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 250}
0.29 (+/-0.104) for {'learning_rate': 100, 'max_depth': 3, 'n_estimators': 500}
0.373 (+/-0.181) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 5}
0.375 (+/-0.174) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 50}
0.369 (+/-0.176) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 250}
0.375 (+/-0.173) for {'learning_rate': 100, 'max_depth': 5, 'n_estimators': 500}
0.551 (+/-0.126) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 5}
0.547 (+/-0.13) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 50}
0.584 (+/-0.117) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 250}
0.562 (+/-0.135) for {'learning_rate': 100, 'max_depth': 7, 'n_estimators': 500}
0.635 (+/-0.063) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 5}
0.674 (+/-0.079) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 50}
0.652 (+/-0.061) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 250}
0.663 (+/-0.108) for {'learning_rate': 100, 'max_depth': 9, 'n_estimators': 500}

joblib.dump(cv.best_estimator_, '../../../GB_model.pkl')

************************************************************************************************************
7. SUMMARY
************************************************************************************************************
WHY DO YOU NEED TO CONSIDER SO MANY DIFFERENT MODELS?
CONCEPTUAL COMPARISON OF ALGORITHMS
FINAL MODEL SELECTION AND EVALUATION


val_features = pd.read_csv('../../../val_features.csv')
val_labels = pd.read_csv('../../../val_labels.csv', header=None)

te_features = pd.read_csv('../../../test_features.csv')
te_labels = pd.read_csv('../../../test_labels.csv', header=None)


import joblib
models = {}

for mdl in ['LR', 'SVM', 'MLP', 'RF', 'GB']:
    models[mdl] = joblib.load('../../../{}_model.pkl'.format(mdl))


def evaluate_model(name, model, features, labels):
    start = time()
    pred = model.predict(features)
    end = time()
    accuracy = round(accuracy_score(labels, pred), 3)
    precision = round(precision_score(labels, pred), 3)
    recall = round(recall_score(labels, pred), 3)
    print('{} -- Accuracy: {} / Precision: {} / Recall: {} / Latency: {}ms'.format(name,
                                                                                   accuracy,
                                                                                   precision,
                                                                                   recall,
                                                                                   round((end - start)*1000, 1)))


for name, mdl in models.items():
    evaluate_model(name, mdl, val_features, val_labels)
>>>
LR -- Accuracy: 0.77 / Precision: 0.707 / Recall: 0.631 / Latency: 1.5ms
SVM -- Accuracy: 0.747 / Precision: 0.672 / Recall: 0.6 / Latency: 1.4ms
MLP -- Accuracy: 0.747 / Precision: 0.667 / Recall: 0.615 / Latency: 1.2ms
RF -- Accuracy: 0.82 / Precision: 0.824 / Recall: 0.646 / Latency: 7.0ms
GB -- Accuracy: 0.815 / Precision: 0.808 / Recall: 0.646 / Latency: 2.4ms
#Result 2
LR -- Accuracy: 0.77 / Precision: 0.707 / Recall: 0.631 / Latency: 0.0ms
SVM -- Accuracy: 0.747 / Precision: 0.672 / Recall: 0.6 / Latency: 15.7ms
MLP -- Accuracy: 0.781 / Precision: 0.697 / Recall: 0.708 / Latency: 0.0ms
RF -- Accuracy: 0.809 / Precision: 0.83 / Recall: 0.6 / Latency: 15.6ms
GB -- Accuracy: 0.815 / Precision: 0.808 / Recall: 0.646 / Latency: 0.0ms


evaluate_model('Random Forest', models['RF'], te_features, te_labels)
>>>
Random Forest -- Accuracy: 0.81 / Precision: 0.875 / Recall: 0.645 / Latency: 7.5ms

evaluate_model('GB', models['GB'], te_features, te_labels)
>>>
GB -- Accuracy: 0.816 / Precision: 0.852 / Recall: 0.684 / Latency: 0.0ms